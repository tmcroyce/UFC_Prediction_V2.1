{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Sherdog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a web scraping script in Python that uses Selenium and Beautiful Soup to extract profile and fight history data of a specific fighter from a web page (https://www.sherdog.com/).\n",
    "\n",
    "1. The script first loads the webpage using Selenium's webdriver and pauses for 5 seconds to ensure the entire page has fully loaded.\n",
    "\n",
    "2. Beautiful Soup then parses the source code of the webpage and a specific div with class 'fighter-data' is targetted where most of the information about the fighter is stored. \n",
    "\n",
    "3. It extracts various details about the fighter like their birthdate, height, weight, association, class, and their match stats (wins, losses, etc.).\n",
    "\n",
    "4. It prints all this information.\n",
    "\n",
    "5. This data is then stored in a dictionary and converted into a pandas DataFrame (df1) consisting of a single row (since it's information about just one fighter).\n",
    "\n",
    "6. The function then scrapes the fighter's match history from a table on the web page, iterating over each row (representing a previously fought match), and extracts specific details about each match (result, opponent, event, date, method, referee, round, time).\n",
    "\n",
    "7. Each match's data is stored in a dictionary and all match dictionaries are gathered into a list.\n",
    "\n",
    "8. This list is then converted into a pandas DataFrame and both the dataframes are saved as CSV files. One for the fighter's profile info and the other for their match history.\n",
    "\n",
    "Note: Throughout the program, 'try-except' blocks are used to handle cases where certain pieces of information may be missing from the page so that the code doesn't break.\n",
    "\n",
    "In summary, this script automates the process of queuing up a fighter's profile page, scraping the page to extract the fighter's profile and match history data, and saving these details into CSV files. You would need to ensure that you have permission to scrape and use the data from the website in this way, to comply with the site's terms of service and relevant law. This code would likely be part of a larger project, perhaps for data analysis on fighters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib.ticker as mtick\n",
    "import sqlite3\n",
    "import seaborn as sns\n",
    "from matplotlib.pyplot import figure\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import requests     # to get images\n",
    "import shutil       # to save files locally\n",
    "import datetime\n",
    "from scipy.stats import norm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import xgboost\n",
    "from xgboost import XGBClassifier\n",
    "from random import randint\n",
    "import  random\n",
    "import os\n",
    "os.chdir('C:/Users/Travis/OneDrive/Data Science/Personal_Projects/Sports/UFC_Prediction_V2')\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from cmath import nan\n",
    "import urllib\n",
    "import urllib.request\n",
    "import re\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get sitemap for fighters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = 'https://www.sherdog.com/sitemap-fighters.xml'\n",
    "# header = {\n",
    "#   \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.75 Safari/537.36\",\n",
    "#   \"X-Requested-With\": \"XMLHttpRequest\"\n",
    "# }\n",
    "# r = requests.get(url, headers = header)\n",
    "# r.text\n",
    "\n",
    "# # turn into df\n",
    "# df = pd.DataFrame(r.text.split('\\n'))\n",
    "# df.columns = ['url']\n",
    "# df = df[df['url'].str.contains('fighter')]\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # fighter names, remove  numbers at end of names\n",
    "# df['fighter_first'] = df['url'].str.split('fighter/').str[1].replace('<loc></url>', \"\").str.split('-').str[0]\n",
    "# df['fighter_last'] = df['url'].str.split('fighter/').str[1].replace('<loc></url>', \"\").str.split('-').str[-2]\n",
    "\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['sherdog_urls'] = df['url'].str.replace('<loc>', \"\").str.replace('<url>', '').str.strip()\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df= df.drop(['url'], axis = 1)\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('data/final/aggregates/Sherdog_Fighter_URLs.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sherdog_fighter_urls = pd.read_csv('data/final/aggregates/Sherdog_Fighter_URLs.csv')\n",
    "sherdog_fighter_urls"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Fighter Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PROBLEM (4.3.23)\n",
    "\n",
    "Need to find the iframe Sources (such as: 'https://www.ufc.com/matchup/1135/10377/pre')\n",
    "\n",
    "To do this, need to:\n",
    "- A) go to each matchup\n",
    "- B) get IN the html\n",
    "- C) Find the iframe Src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_events(url):\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    # get events\n",
    "    event1 = soup.find('div', class_='c-card-event--result__info')\n",
    "    event1_txt = soup.find('div', class_='c-card-event--result__info').text\n",
    "    event1_url = event1.find('a')['href']\n",
    "    event1_url = 'https://www.ufc.com' + event1_url\n",
    "    event1_title = event1_txt.split('\\n')[1]\n",
    "    event1_time = event1_txt.split('/')[1]\n",
    "\n",
    "    data = pd.DataFrame({'event_title': [event1_title], 'event_url': [event1_url], 'event_date': [event1_time]})\n",
    "\n",
    "    event2 = soup.find('div', class_='c-card-event--result__info').find_next('div', class_='c-card-event--result__info')\n",
    "    event2_txt = soup.find('div', class_='c-card-event--result__info').find_next('div', class_='c-card-event--result__info').text\n",
    "    event2_url = event2.find('a')['href']\n",
    "    event2_url = 'https://www.ufc.com' + event2_url\n",
    "    event2_title = event2_txt.split('\\n')[1]\n",
    "    event2_time = event2_txt.split('/')[1]\n",
    "\n",
    "\n",
    "    data = data.append({'event_title': event2_title, 'event_url': event2_url, 'event_date': event2_time}, ignore_index=True)\n",
    "    \n",
    "    event3 = soup.find('div', class_='c-card-event--result__info').find_next('div', class_='c-card-event--result__info').find_next('div', class_='c-card-event--result__info')\n",
    "    event3_txt = soup.find('div', class_='c-card-event--result__info').find_next('div', class_='c-card-event--result__info').find_next('div', class_='c-card-event--result__info').text\n",
    "    event3_url = event3.find('a')['href']\n",
    "    event3_url = 'https://www.ufc.com' + event3_url\n",
    "    event3_title = event3_txt.split('\\n')[1]\n",
    "    event3_time = event3_txt.split('/')[1]\n",
    "\n",
    "    data = data.append({'event_title': event3_title, 'event_url': event3_url, 'event_date': event3_time}, ignore_index=True)\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Function to get the fight card for a given event using BeautifulSoup\n",
    "def get_event_fights(event_url):\n",
    "    page = requests.get(event_url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    # get main card, fight 1\n",
    "\n",
    "    mcn = soup.find_all('li', class_='l-listing__item')\n",
    "    # get num of mc\n",
    "    num_mc = len(mcn)\n",
    "    # for each mc, do the following\n",
    "    data = pd.DataFrame()\n",
    "    n = 0\n",
    "    for i in mcn:\n",
    "        mc = mcn[n]\n",
    "        # fight 1\n",
    "        fighter1= mc.find('div', class_ ='c-listing-fight__corner-name c-listing-fight__corner-name--red').text\n",
    "        fighter1 = fighter1.replace('\\n', ' ')\n",
    "        fighter1 = fighter1.strip()\n",
    "        fighter2 = mc.find('div', class_ ='c-listing-fight__corner-name c-listing-fight__corner-name--blue').text\n",
    "        fighter2 = fighter2.replace('\\n', ' ')\n",
    "        fighter2 = fighter2.strip()\n",
    "        weightclass = mc.find('div', class_='c-listing-fight__class-text').text\n",
    "        fighter1_odds = mc.find('span', class_='c-listing-fight__odds').text\n",
    "        fighter2_odds = mc.find('span', class_='c-listing-fight__odds').find_next('span', class_='c-listing-fight__odds').text\n",
    "        fighter1_odds = fighter1_odds.replace('\\n', '')\n",
    "        fighter2_odds = fighter2_odds.replace('\\n', '')\n",
    "        # fighter odds to float\n",
    "        if (fighter1_odds == '-') :\n",
    "            fighter1_odds = nan\n",
    "        if (fighter2_odds == '-') :\n",
    "            fighter2_odds = nan\n",
    "\n",
    "        data = data.append({'fighter1': fighter1, 'fighter2': fighter2, 'weightclass': weightclass, \n",
    "                            'fighter1_odds': fighter1_odds, 'fighter2_odds': fighter2_odds}, ignore_index=True)\n",
    "        n = n + 1\n",
    "    return data\n",
    "\n",
    "# Find secret number in ufc events using BS & Selenium\n",
    "def secret_number(event_url):\n",
    "    # if no driver open, open one\n",
    "    driver = None\n",
    "    if (driver == None):\n",
    "        driver = webdriver.Chrome('C:\\\\Users\\\\Travis\\\\OneDrive\\\\Data Science\\\\Personal_Projects\\\\Sports\\\\UFC_Prediction_V2\\\\chromedriver.exe')\n",
    "    else:\n",
    "        driver = driver\n",
    "    \n",
    "    driver.get(event_url)\n",
    "    time.sleep(3)\n",
    "    # click the first matchup\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    pretty = soup.prettify()\n",
    "    # find first data-fmid to get first matchup\n",
    "    fmid_start = pretty.find('data-fmid')\n",
    "    fmid = pretty[fmid_start+11:fmid_start+16]\n",
    "    driver.get(event_url +'#' + fmid)\n",
    "    time.sleep(6)\n",
    "    # find all links within page\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    # find all iframe src\n",
    "    iframe = soup.find_all('iframe')\n",
    "    # find all links\n",
    "    iframe_text = str(iframe)\n",
    "    matchup = iframe_text.find('matchup')\n",
    "    matchup_url = iframe_text[matchup+8:matchup+12]\n",
    "    print('matchup_url: ' + matchup_url)\n",
    "    secret_number = matchup_url\n",
    "    return matchup\n",
    "\n",
    "# get next events if event fighter data is not na\n",
    "def get_next_events2(url):\n",
    "    data = get_next_events(url)\n",
    "    for i in range(0, len(data)):\n",
    "        event_url = data['event_url'][i]\n",
    "        event_fights = get_event_fights(event_url)\n",
    "        if (len(event_fights) == 0):\n",
    "            data = data.drop(i)\n",
    "    return data\n",
    "\n",
    "# get next events from UFCStats.com using BS\n",
    "def get_next_event_ufcstats():\n",
    "    url = 'http://www.ufcstats.com/statistics/events/upcoming'\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    # get events\n",
    "    event1 = soup.find('td', class_='b-statistics__table-col')\n",
    "    event1_txt = soup.find('td', class_='b-statistics__table-col').text\n",
    "    event_txt = event1_txt.replace('   ', '').replace('\\n', '').strip()\n",
    "    event_title = event_txt.split('  ')[0]\n",
    "    event_date = event_txt.split('  ')[1]\n",
    "    event1_url = event1.find('a')['href']\n",
    "    data = pd.DataFrame({'event_title': [event_title], 'event_url': [event1_url], 'event_date': [event_date]})\n",
    "    return data\n",
    "\n",
    "\n",
    "# get fighter urls from UFCStats.com using BS\n",
    "def get_fighter_urls(event_details_url):\n",
    "    page = requests.get(event_details_url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    # get events\n",
    "    events = soup.find_all('tr', class_='b-fight-details__table-row b-fight-details__table-row__hover js-fight-details-click')\n",
    "    n = 0\n",
    "    next_event_data = pd.DataFrame()\n",
    "\n",
    "    for event in events:\n",
    "        fighters = events[n].find_all('p', class_='b-fight-details__table-text')\n",
    "        fighter1 = fighters[0].text\n",
    "        fighter1 = fighter1.replace('  ', '').replace('\\n', '').strip()\n",
    "        fighter2 = fighters[1].text\n",
    "        fighter2 = fighter2.replace('  ', '').replace('\\n', '').strip()\n",
    "        fighter1_url = fighters[0].find('a')['href']\n",
    "        fighter2_url = fighters[1].find('a')['href']\n",
    "        next_event_data = next_event_data.append({'fighter1' :fighter1, 'fighter2:' : fighter2, 'fighter1_url': fighter1_url, 'fighter2_url':fighter2_url, 'fight#' : n+1}, ignore_index = True)\n",
    "        n += 1\n",
    "\n",
    "    return next_event_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_events = get_next_events2('https://www.ufc.com/events')\n",
    "next_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_url =  next_events['event_url'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_event_title = next_events['event_title'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_event = get_event_fights(event_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = requests.get(event_url)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "h = soup.find_all('div', class_='c-listing-fight')\n",
    "\n",
    "data_fmid = []\n",
    "for i in h:\n",
    "    data_fmid.append(i['data-fmid'])\n",
    "\n",
    "next_event['fight_number'] = data_fmid[:len(next_event)]\n",
    "next_event['matchup_url'] = event_url +'#' + next_event['fight_number'].astype(str)\n",
    "\n",
    "next_event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_iframe_sources(matchup_url):\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(matchup_url)\n",
    "    time.sleep(3)\n",
    "    # get innerhtml\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    pretty = soup.prettify()\n",
    "    # find all links within page\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    # find all iframe src\n",
    "    iframe = soup.find_all('iframe')\n",
    "    # list all links\n",
    "    iframe_text = str(iframe)\n",
    "    # separate links\n",
    "    iframe_text = iframe_text.split('src=\"')\n",
    "    iframe_text = iframe_text[1:]\n",
    "    iframe_text = [i.split('\"')[0] for i in iframe_text]\n",
    "    # only keep links that contain matchup\n",
    "    iframe_text = [i for i in iframe_text if 'matchup' in i]\n",
    "    # only keep top link\n",
    "    iframe_text = iframe_text[0]\n",
    "    \n",
    "\n",
    "\n",
    "    return iframe_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_all_iframe_sources(next_event['matchup_url'][1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add iframe Sources to next_event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add iframe sources to df  with itterrows and apply\n",
    "next_event['iframe_src'] = next_event.apply(lambda x: find_all_iframe_sources(x['matchup_url']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_event['full_iframe_src'] = 'https://www.ufc.com' + next_event['iframe_src']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_iframe_src_data_2(iframe_src):\n",
    "    iframe_response = requests.get(iframe_src)\n",
    "    iframe_soup = BeautifulSoup(iframe_response.content, 'html.parser')\n",
    "\n",
    "    images = iframe_soup.find_all('img', class_='fighter-img--large-screen')\n",
    "\n",
    "    # get red fighter image.\n",
    "    red_fighter_image = images[0]['src']\n",
    "    blue_fighter_image = images[1]['src']\n",
    "    \n",
    "\n",
    "    # get all c-stat-compare__group-1 red\n",
    "    red = iframe_soup.find_all('div', class_='c-stat-compare__group-1 red')\n",
    "    # return all red texts\n",
    "    red_text = [i.text for i in red]\n",
    "    # assign to variables\n",
    "    red_record = red_text[0]\n",
    "    red_last_fight = red_text[1]\n",
    "    red_country = red_text[2]\n",
    "    red_height = red_text[3]\n",
    "    red_weight = red_text[4]\n",
    "    red_reach = red_text[5]\n",
    "    red_legreach = red_text[6]\n",
    "    red_win_by_ko_percent = red_text[7]\n",
    "    red_win_by_sub_percent = red_text[8]\n",
    "    red_win_by_dec_percent = red_text[9]\n",
    "    red_avg_fight_time = red_text[10]\n",
    "    red_knockdowns_per_15_min = red_text[11]\n",
    "    # sig strikes\n",
    "    red_sig_strikes_landed_per_min = red_text[12]\n",
    "    red_sig_strikes_percent = red_text[13]\n",
    "    red_sig_strikes_absorbed_per_min = red_text[14]\n",
    "    red_sig_strikes_absorbed_percent = red_text[15]\n",
    "    # grappling\n",
    "    red_takedowns_landed_per_15_min = red_text[16]\n",
    "    red_takedown_accuracy = red_text[17]\n",
    "    red_takedown_defense = red_text[18]\n",
    "    red_submissions_attempts_per_15_min = red_text[19]\n",
    "    # odds\n",
    "    red_moneyline = red_text[20]\n",
    "\n",
    "\n",
    "    # make df for red\n",
    "    red_df = pd.DataFrame({'red_record': red_record, 'red_last_fight': red_last_fight, \n",
    "                            'red_country': red_country, 'red_height': red_height, 'red_weight': red_weight, \n",
    "                            'red_reach': red_reach, 'red_legreach': red_legreach, \n",
    "                            'red_win_by_ko_percent': red_win_by_ko_percent, \n",
    "                            'red_win_by_sub_percent': red_win_by_sub_percent, 'red_win_by_dec_percent': red_win_by_dec_percent, \n",
    "                            'red_avg_fight_time': red_avg_fight_time, 'red_knockdowns_per_15_min': red_knockdowns_per_15_min, \n",
    "                            'red_sig_strikes_landed_per_min': red_sig_strikes_landed_per_min, \n",
    "                            'red_sig_strikes_percent': red_sig_strikes_percent, \n",
    "                            'red_sig_strikes_absorbed_per_min': red_sig_strikes_absorbed_per_min, \n",
    "                            'red_sig_strikes_absorbed_percent': red_sig_strikes_absorbed_percent, \n",
    "                            'red_takedowns_landed_per_15_min': red_takedowns_landed_per_15_min, \n",
    "                            'red_takedown_accuracy': red_takedown_accuracy, 'red_takedown_defense': red_takedown_defense, \n",
    "                            'red_submissions_attempts_per_15_min': red_submissions_attempts_per_15_min, \n",
    "                            'red_moneyline': red_moneyline, 'red_fighter_image' : red_fighter_image}, index=[0])\n",
    "\n",
    "    # clean all values in red_df, removing all \\n \n",
    "    red_df = red_df.applymap(lambda x: x.replace('\\n', ''))\n",
    "    # strip all values in red_df\n",
    "    red_df = red_df.applymap(lambda x: x.strip())\n",
    "\n",
    "    # get all c-stat-compare__group-1 blue\n",
    "    blue = iframe_soup.find_all('div', class_='c-stat-compare__group-2 blue')\n",
    "    # return all blue texts\n",
    "    blue_text = [i.text for i in blue]\n",
    "    # assign to variables\n",
    "    blue_record = blue_text[0]\n",
    "    blue_last_fight = blue_text[1]\n",
    "    blue_country = blue_text[2]\n",
    "    blue_height = blue_text[3]\n",
    "    blue_weight = blue_text[4]\n",
    "    blue_reach = blue_text[5]\n",
    "    blue_legreach = blue_text[6]\n",
    "    blue_win_by_ko_percent = blue_text[7]\n",
    "    blue_win_by_sub_percent = blue_text[8]\n",
    "    blue_win_by_dec_percent = blue_text[9]\n",
    "    blue_avg_fight_time = blue_text[10]\n",
    "    blue_knockdowns_per_15_min = blue_text[11]\n",
    "    # sig strikes\n",
    "    blue_sig_strikes_landed_per_min = blue_text[12]\n",
    "    blue_sig_strikes_percent = blue_text[13]\n",
    "    blue_sig_strikes_absorbed_per_min = blue_text[14]\n",
    "    blue_sig_strikes_absorbed_percent = blue_text[15]\n",
    "    # grappling\n",
    "    blue_takedowns_landed_per_15_min = blue_text[16]\n",
    "    blue_takedown_accuracy = blue_text[17]\n",
    "    blue_takedown_defense = blue_text[18]\n",
    "    blue_submissions_attempts_per_15_min = blue_text[19]\n",
    "    # odds\n",
    "    blue_moneyline = blue_text[20]\n",
    "\n",
    "\n",
    "    # make df for blue\n",
    "    blue_df = pd.DataFrame({'blue_record': blue_record, 'blue_last_fight': blue_last_fight,\n",
    "                            'blue_country': blue_country, 'blue_height': blue_height, 'blue_weight': blue_weight,\n",
    "                            'blue_reach': blue_reach, 'blue_legreach': blue_legreach,\n",
    "                            'blue_win_by_ko_percent': blue_win_by_ko_percent,\n",
    "                            'blue_win_by_sub_percent': blue_win_by_sub_percent, 'blue_win_by_dec_percent': blue_win_by_dec_percent,\n",
    "                            'blue_avg_fight_time': blue_avg_fight_time, 'blue_knockdowns_per_15_min': blue_knockdowns_per_15_min,\n",
    "                            'blue_sig_strikes_landed_per_min': blue_sig_strikes_landed_per_min,\n",
    "                            'blue_sig_strikes_percent': blue_sig_strikes_percent,\n",
    "                            'blue_sig_strikes_absorbed_per_min': blue_sig_strikes_absorbed_per_min,\n",
    "                            'blue_sig_strikes_absorbed_percent': blue_sig_strikes_absorbed_percent,\n",
    "                            'blue_takedowns_landed_per_15_min': blue_takedowns_landed_per_15_min,\n",
    "                            'blue_takedown_accuracy': blue_takedown_accuracy, 'blue_takedown_defense': blue_takedown_defense,\n",
    "                            'blue_submissions_attempts_per_15_min': blue_submissions_attempts_per_15_min,\n",
    "                            'blue_moneyline': blue_moneyline, 'blue_fighter_image': blue_fighter_image}, index=[0])\n",
    "\n",
    "    # clean all values in blue_df, removing all \\n\n",
    "    blue_df = blue_df.applymap(lambda x: x.replace('\\n', ''))\n",
    "    # strip all values in blue_df\n",
    "    blue_df = blue_df.applymap(lambda x: x.strip())\n",
    "\n",
    "    # # transpose both dfs, then merge]\n",
    "    # red_df = red_df.T\n",
    "    # # rename column 'red'\n",
    "    # red_df.columns = ['red']\n",
    "    # # rename rows, removing 'red_'\n",
    "    # red_df.index = red_df.index.str.replace('red_', '')\n",
    "    # # transpose blue_df\n",
    "    # blue_df = blue_df.T\n",
    "    # # rename column 'blue'\n",
    "    # blue_df.columns = ['blue']\n",
    "    # # rename rows, removing 'blue_'\n",
    "    # blue_df.index = blue_df.index.str.replace('blue_', '')\n",
    "\n",
    "    # # merge red_df and blue_df\n",
    "    # df = pd.concat([red_df, blue_df], axis=1)\n",
    "\n",
    "    # append blue_df to red_df by axis=1\n",
    "    dfs = pd.concat([red_df, blue_df], axis=1)\n",
    "\n",
    "    # return df\n",
    "    return dfs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_iframe_src_data_2(next_event['full_iframe_src'][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get iframe src data for all events\n",
    "\n",
    "event_data = []\n",
    "\n",
    "for i in range(len(next_event['full_iframe_src'])):\n",
    "    event_data.append(get_iframe_src_data_2(next_event['full_iframe_src'][i]))\n",
    "\n",
    "# make df from event_data\n",
    "event_data_df = pd.concat(event_data, axis=0)\n",
    "\n",
    "event_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reindex both\n",
    "next_event = next_event.reset_index(drop=True)\n",
    "event_data_df = event_data_df.reset_index(drop=True)\n",
    "\n",
    "# Append event_data_df to next_event\n",
    "next_event = pd.concat([next_event, event_data_df], axis=1)\n",
    "next_event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list red fighter images\n",
    "red_fighter_images = next_event['red_fighter_image'].tolist()\n",
    "# keep only if 'athlete' in string\n",
    "red_fighter_images = [i for i in red_fighter_images if 'athlete' in i]\n",
    "red_fighter_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list blue fighter images\n",
    "blue_fighter_images = next_event['blue_fighter_image'].tolist()\n",
    "# keep only if 'athlete' in string\n",
    "blue_fighter_images = [i for i in blue_fighter_images if 'athlete' in i]\n",
    "blue_fighter_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download all images in red_fighter_image and blue_fighter_image, save as fighter_name.png\n",
    "\n",
    "# red_fighter_image\n",
    "for i in range(len(next_event['red_fighter_image'])):\n",
    "    if 'athlete' in next_event['red_fighter_image'][i]:\n",
    "        # get url\n",
    "        url = next_event['red_fighter_image'][i]\n",
    "        # get fighter name\n",
    "        fighter_name = next_event['fighter1'][i]\n",
    "        # get image\n",
    "        image = requests.get(url)\n",
    "        # save image\n",
    "        with open(f'data/fighter_images/{fighter_name}.png', 'wb') as f:\n",
    "            f.write(image.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# blue_fighter_image\n",
    "for i in range(len(next_event['blue_fighter_image'])):\n",
    "    if 'athlete' in next_event['blue_fighter_image'][i]:\n",
    "        # get url\n",
    "        url = next_event['blue_fighter_image'][i]\n",
    "        # get fighter name\n",
    "        fighter_name = next_event['fighter2'][i]\n",
    "        # get image\n",
    "        image = requests.get(url)\n",
    "        # save image\n",
    "        with open(f'data/fighter_images/{fighter_name}.png', 'wb') as f:\n",
    "            f.write(image.content)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download images from matchup_url\n",
    "driver = None\n",
    "if driver == None:\n",
    "    driver = webdriver.Chrome('C:\\\\Users\\\\Travis\\\\OneDrive\\\\Data Science\\\\Personal_Projects\\\\Sports\\\\UFC_Prediction_V2\\\\chromedriver.exe')\n",
    "driver.get(selected_matchup_url)\n",
    "time.sleep(2)\n",
    "\n",
    "\n",
    "\n",
    "soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "h = soup.find_all('img')\n",
    "imgs = []\n",
    "for i in h:\n",
    "    imgs.append(i['src'])\n",
    "\n",
    "# keep imgs with full_body\n",
    "imgs = [i for i in imgs if 'full_body' in i]\n",
    "\n",
    "fighter_img1 = [i for i in imgs if fighter_last_name1 in i][0]\n",
    "fighter_img1 = fighter_img1.replace('athlete_detail_stance_thumbnail_full_body', 'athlete_matchup_stats_full_body')\n",
    "# end fighter_img at .png\n",
    "fighter_img1 = fighter_img1[:fighter_img1.find('.png') + 4]\n",
    "\n",
    "fighter_img2 = [i for i in imgs if fighter_last_name2 in i][0]\n",
    "fighter_img2 = fighter_img2.replace('athlete_detail_stance_thumbnail_full_body', 'athlete_matchup_stats_full_body')\n",
    "# end fighter_img at .png\n",
    "fighter_img2 = fighter_img2[:fighter_img2.find('.png') + 4]\n",
    "\n",
    "\n",
    "# save fighter_img1 to file\n",
    "fighter1_img = requests.get(fighter_img1)\n",
    "with open(home + 'fighter_images/' + selected_fighter_1 + '.png', 'wb') as f:\n",
    "    f.write(fighter1_img.content)\n",
    "\n",
    "\n",
    "fighter2_img = requests.get(fighter_img2)\n",
    "with open(home + 'fighter_images/' + selected_fighter_2 + '.png', 'wb') as f:\n",
    "    f.write(fighter2_img.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download all fighter photos\n",
    "\n",
    "selected_matchup_url = next_event['matchup_url'].values[1]\n",
    "\n",
    "def get_fighter_pic_url(selected_matchup_url):\n",
    "    fighter_last_name1 = selected_fighter_1.split(' ')[1]\n",
    "    fighter_last_name1 = fighter_last_name1.upper()\n",
    "    fighter_first_name1 = selected_fighter_1.split(' ')[0]\n",
    "    fighter_first_name1 = fighter_first_name1.upper()\n",
    "\n",
    "    fighter_last_name2 = selected_fighter_2.split(' ')[1]\n",
    "    fighter_last_name2 = fighter_last_name2.upper()\n",
    "    fighter_first_name2 = selected_fighter_2.split(' ')[0]\n",
    "    fighter_first_name2 = fighter_first_name2.upper()\n",
    "\n",
    "    driver = None\n",
    "    if driver == None:\n",
    "        driver = webdriver.Chrome('C:\\\\Users\\\\Travis\\\\OneDrive\\\\Data Science\\\\Personal_Projects\\\\Sports\\\\UFC_Prediction_V2\\\\chromedriver.exe')\n",
    "    driver.get(selected_matchup_url)\n",
    "    time.sleep(2)\n",
    "    \n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    h = soup.find_all('img')\n",
    "    imgs = []\n",
    "    for i in h:\n",
    "        imgs.append(i['src'])\n",
    "\n",
    "    # keep imgs with full_body\n",
    "    imgs = [i for i in imgs if 'full_body' in i]\n",
    "\n",
    "    fighter_img1 = [i for i in imgs if fighter_last_name1 in i][0]\n",
    "    fighter_img1 = fighter_img1.replace('athlete_detail_stance_thumbnail_full_body', 'athlete_matchup_stats_full_body')\n",
    "    # end fighter_img at .png\n",
    "    fighter_img1 = fighter_img1[:fighter_img1.find('.png') + 4]\n",
    "\n",
    "    fighter_img2 = [i for i in imgs if fighter_last_name2 in i][0]\n",
    "    fighter_img2 = fighter_img2.replace('athlete_detail_stance_thumbnail_full_body', 'athlete_matchup_stats_full_body')\n",
    "    # end fighter_img at .png\n",
    "    fighter_img2 = fighter_img2[:fighter_img2.find('.png') + 4]\n",
    "\n",
    "\n",
    "    # save fighter_img1 to file\n",
    "    fighter1_img = requests.get(fighter_img1)\n",
    "    with open(home + 'fighter_images/' + selected_fighter_1 + '.png', 'wb') as f:\n",
    "        f.write(fighter1_img.content)\n",
    "    \n",
    "\n",
    "    fighter2_img = requests.get(fighter_img2)\n",
    "    with open(home + 'fighter_images/' + selected_fighter_2 + '.png', 'wb') as f:\n",
    "        f.write(fighter2_img.content)\n",
    "        \n",
    "        #return fighter_img2\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_fighter_pic_url(selected_matchup_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to csv\n",
    "next_event.to_csv('data/final/next_fights/'+ next_event_title +'_.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape Sherdog for Individual Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.sherdog.com/fighter/Israel-Adesanya-56374'\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(url)\n",
    "time.sleep(5)\n",
    "\n",
    "soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "fighter_data = soup.find('div', {'class': 'fighter-data'})\n",
    "fighter_name = soup.find('h1', itemprop='name').text\n",
    "# Scraping the data\n",
    "\n",
    "birthdate = fighter_data.find('span', itemprop='birthDate').text\n",
    "height = fighter_data.find('b', itemprop='height').text\n",
    "weight = fighter_data.find('b', itemprop='weight').text\n",
    "association = fighter_data.find('span', itemprop='name').text\n",
    "fight_class = fighter_data.find('a', {'href': '/stats/fightfinder?weightclass=Middleweight'}).text\n",
    "\n",
    "wins = fighter_data.find('div', {'class': 'winloses win'}).find('span', class_=None).text\n",
    "ko_tko_wins = fighter_data.find('div', {'class': 'wins'}).find('div', class_='pl').text\n",
    "submission_wins = fighter_data.find('div', {'class': 'wins'}).find_all('div', class_='pl')[1].text\n",
    "decision_wins = fighter_data.find('div', {'class': 'wins'}).find_all('div', class_='pl')[2].text\n",
    "\n",
    "losses = fighter_data.find('div', {'class': 'winloses lose'}).find('span', class_=None).text\n",
    "ko_tko_losses = fighter_data.find('div', {'class': 'loses'}).find('div', class_='pl').text\n",
    "submission_losses = fighter_data.find('div', {'class': 'loses'}).find_all('div', class_='pl')[1].text\n",
    "decision_losses = fighter_data.find('div', {'class': 'loses'}).find_all('div', class_='pl')[2].text\n",
    "\n",
    "# Printing the data\n",
    "print(f\"Name: {fighter_name}\")\n",
    "print(f\"Birthdate: {birthdate}\")\n",
    "print(f\"Height: {height}\")\n",
    "print(f\"Weight: {weight}\")\n",
    "print(f\"Association: {association}\")\n",
    "print(f\"Class: {fight_class}\")\n",
    "print(f\"Wins: {wins}\")\n",
    "print(f\"  KO/TKO: {ko_tko_wins}\")\n",
    "print(f\"  Submission: {submission_wins}\")\n",
    "print(f\"  Decision: {decision_wins}\")\n",
    "print(f\"Losses: {losses}\")\n",
    "print(f\"  KO/TKO: {ko_tko_losses}\")\n",
    "print(f\"  Submission: {submission_losses}\")\n",
    "print(f\"  Decision: {decision_losses}\")\n",
    "\n",
    "# put data in df\n",
    "data = {'fighter': fighter_name, 'birthdate': birthdate, 'height': height, \n",
    "        'weight': weight, 'association': association, 'class': fight_class, \n",
    "        'wins': wins, 'ko_tko_wins': ko_tko_wins, 'submission_wins': submission_wins, \n",
    "        'decision_wins': decision_wins, 'losses': losses, 'ko_tko_losses': ko_tko_losses, \n",
    "        'submission_losses': submission_losses, 'decision_losses': decision_losses}\n",
    "\n",
    "df1 = pd.DataFrame(data, index=[0])\n",
    "\n",
    "\n",
    "\n",
    "fighter_history = soup.find('div', {'class': 'new_table_holder'})\n",
    "fight_rows = fighter_history.find_all('tr')[1:] # Excluding the header row\n",
    "fights = []\n",
    "\n",
    "for row in fight_rows:\n",
    "    fight_data = {}\n",
    "    cols = row.find_all('td')\n",
    "    fight_data['fighter'] = fighter_name\n",
    "    \n",
    "    fight_data['result'] = cols[0].text.strip()\n",
    "    fight_data['opponent'] = cols[1].find('a').text.strip()\n",
    "    fight_data['event'] = cols[2].find('a').text.strip()\n",
    "    fight_data['event_date'] = cols[2].find('span', class_='sub_line').text.strip()\n",
    "    fight_data['method'] = cols[3].find('b').text.strip()\n",
    "    try:\n",
    "        fight_data['referee'] = cols[3].find('span', class_='sub_line').find('a').text.strip()\n",
    "    except:\n",
    "        fight_data['referee'] = None\n",
    "    fight_data['round'] = cols[4].text.strip()\n",
    "    fight_data['time'] = cols[5].text.strip()\n",
    "    \n",
    "    fights.append(fight_data)\n",
    "\n",
    "# append all info to a df\n",
    "df = pd.DataFrame(fights)\n",
    "df.to_csv('data/Sherdog/fighters/'+ fighter_name + '_fight_history.csv', index = False)\n",
    "df1.to_csv('data/Sherdog/fighters/'+ fighter_name + '.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('Trav310')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f69e36f0e9b2c8d9f319b417484f14b77c91d7bef950ad448542405eb1e0e594"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
