{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Set Model Version](#toc1_1_)    \n",
    "      - [Check NAN](#toc1_1_1_1_)    \n",
    "      - [Assign Target and Split](#toc1_1_1_2_)    \n",
    "      - [Split](#toc1_1_1_3_)    \n",
    "    - [Preprocessing](#toc1_1_2_)    \n",
    "    - [Function Additions](#toc1_1_3_)    \n",
    "- [Run Vanilla Decision Tree](#toc2_)    \n",
    "    - [Run Vanilla Models](#toc2_1_1_)    \n",
    "    - [Model 1: XGBoost](#toc2_1_2_)    \n",
    "    - [Extra Trees](#toc2_1_3_)    \n",
    "      - [Best Model](#toc2_1_3_1_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib.ticker as mtick\n",
    "import sqlite3\n",
    "import seaborn as sns\n",
    "from matplotlib.pyplot import figure\n",
    "import matplotlib.ticker as ticker\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import graphviz\n",
    "import requests     \n",
    "import shutil       \n",
    "import datetime\n",
    "from scipy.stats import norm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import requests\n",
    "import json\n",
    "import xgboost\n",
    "from xgboost import XGBClassifier\n",
    "from random import randint\n",
    "import  random\n",
    "import os\n",
    "from cmath import nan\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from sklearn.pipeline import Pipeline, make_pipeline, FeatureUnion\n",
    "from sklearn.tree import plot_tree\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.compose import make_column_selector as selector, ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, OrdinalEncoder\n",
    "from sklearn.metrics import fbeta_score\n",
    "# import winsound\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import tree, preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, recall_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import roc_curve, auc, f1_score, make_scorer, recall_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
    "import pprint\n",
    "import pickle\n",
    "from cmath import nan\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "from plotly import graph_objects as go\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.tree import plot_tree\n",
    "import pydot\n",
    "from IPython.display import Image, display\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz, _tree\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "# import lda\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "# import kmeans\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "# # Sound Variables\n",
    "# sound_file = \"data/audio/Jobs Done.wav\"\n",
    "\n",
    "# def play_jobs_done():\n",
    "#     winsound.PlaySound(sound_file, winsound.SND_FILENAME)\n",
    "\n",
    "\n",
    "# bad_sound_file = 'data/audio/Danger Will Robinson.wav'\n",
    "\n",
    "# def play_danger():\n",
    "#     winsound.PlaySound(bad_sound_file, winsound.SND_FILENAME)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_1_'></a>[Set Model Version](#toc0_)\n",
    "\n",
    "If you want to run BRAND NEW Models, set a new model version, which will create a new folder and save the models to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/travisroyce/Library/CloudStorage/OneDrive-Personal/Data Science/Personal_Projects/Sports/UFC_Prediction_V2/models/v51/\n"
     ]
    }
   ],
   "source": [
    "modeling_version = 'v51'\n",
    "\n",
    "home_folder = '/Users/travisroyce/Library/CloudStorage/OneDrive-Personal/Data Science/Personal_Projects/Sports/UFC_Prediction_V2'\n",
    "\n",
    "model_folder = home_folder + '/models/' + modeling_version + '/'\n",
    "print(model_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Folder\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(model_folder):\n",
    "    print('Creating Folder')\n",
    "    os.makedirs(model_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set home folder\n",
    "os.chdir(home_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Fighter_A</th>\n",
       "      <th>Fighter_B</th>\n",
       "      <th>A_Kd</th>\n",
       "      <th>B_Kd</th>\n",
       "      <th>A_Sig_strike_land</th>\n",
       "      <th>A_Sig_strike_att</th>\n",
       "      <th>B_Sig_strike_land</th>\n",
       "      <th>B_Sig_strike_att</th>\n",
       "      <th>A_Sig_strike_percent</th>\n",
       "      <th>...</th>\n",
       "      <th>B_Initial_Martial_Arts_Cats</th>\n",
       "      <th>Ape_Index_Dif</th>\n",
       "      <th>Leg_Index_Dif</th>\n",
       "      <th>Leg_to_Wing_Index_Dif</th>\n",
       "      <th>A_Days_Since_Last_Fight</th>\n",
       "      <th>B_Days_Since_Last_Fight</th>\n",
       "      <th>Days_Since_Last_Fight_Dif</th>\n",
       "      <th>Fighter_A_Odds_Implied_Prob</th>\n",
       "      <th>Fighter_B_Odds_Implied_Prob</th>\n",
       "      <th>Fighter_Odds_Implied_Prob_Dif</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Rick Story</td>\n",
       "      <td>Martin Kampmann</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>147.0</td>\n",
       "      <td>0.35</td>\n",
       "      <td>...</td>\n",
       "      <td>Muay Thai-BJJ</td>\n",
       "      <td>-0.013492</td>\n",
       "      <td>0.001984</td>\n",
       "      <td>0.009326</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>52.38</td>\n",
       "      <td>50.00</td>\n",
       "      <td>2.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Enrique Barzola</td>\n",
       "      <td>Kyle Bochniak</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>0.33</td>\n",
       "      <td>...</td>\n",
       "      <td>BJJ-Muay Thai</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.037313</td>\n",
       "      <td>-0.035714</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>46.51</td>\n",
       "      <td>57.45</td>\n",
       "      <td>-10.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Belal Muhammad</td>\n",
       "      <td>Takashi Sato</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>0.46</td>\n",
       "      <td>...</td>\n",
       "      <td>Karate-Judo</td>\n",
       "      <td>-0.028773</td>\n",
       "      <td>-0.022334</td>\n",
       "      <td>-0.006088</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>79.59</td>\n",
       "      <td>23.81</td>\n",
       "      <td>55.78</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 1079 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0        Fighter_A        Fighter_B  A_Kd  B_Kd  \\\n",
       "0           0       Rick Story  Martin Kampmann   0.0   0.0   \n",
       "1           1  Enrique Barzola    Kyle Bochniak   0.0   0.0   \n",
       "2           2   Belal Muhammad     Takashi Sato   0.0   0.0   \n",
       "\n",
       "   A_Sig_strike_land  A_Sig_strike_att  B_Sig_strike_land  B_Sig_strike_att  \\\n",
       "0               61.0             170.0               38.0             147.0   \n",
       "1               55.0             165.0               41.0             141.0   \n",
       "2               49.0             106.0               29.0              89.0   \n",
       "\n",
       "   A_Sig_strike_percent  ...  B_Initial_Martial_Arts_Cats  Ape_Index_Dif  \\\n",
       "0                  0.35  ...                Muay Thai-BJJ      -0.013492   \n",
       "1                  0.33  ...                BJJ-Muay Thai       0.000000   \n",
       "2                  0.46  ...                  Karate-Judo      -0.028773   \n",
       "\n",
       "   Leg_Index_Dif  Leg_to_Wing_Index_Dif  A_Days_Since_Last_Fight  \\\n",
       "0       0.001984               0.009326                      NaN   \n",
       "1      -0.037313              -0.035714                      NaN   \n",
       "2      -0.022334              -0.006088                      NaN   \n",
       "\n",
       "   B_Days_Since_Last_Fight  Days_Since_Last_Fight_Dif  \\\n",
       "0                      NaN                        NaN   \n",
       "1                      NaN                        NaN   \n",
       "2                      NaN                        NaN   \n",
       "\n",
       "   Fighter_A_Odds_Implied_Prob  Fighter_B_Odds_Implied_Prob  \\\n",
       "0                        52.38                        50.00   \n",
       "1                        46.51                        57.45   \n",
       "2                        79.59                        23.81   \n",
       "\n",
       "   Fighter_Odds_Implied_Prob_Dif  \n",
       "0                           2.38  \n",
       "1                         -10.94  \n",
       "2                          55.78  \n",
       "\n",
       "[3 rows x 1079 columns]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data \n",
    "df  = pd.read_csv('data/final/aggregates/Double_Fights_DF_V20.csv') \n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Lightweight              1582\n",
       "Welterweight             1428\n",
       "Middleweight             1096\n",
       "Featherweight             888\n",
       "Bantamweight              770\n",
       "Light Heavyweight         676\n",
       "Heavyweight               668\n",
       "Flyweight                 396\n",
       "Women's Strawweight       298\n",
       "Women's Bantamweight      250\n",
       "Women's Flyweight         216\n",
       "Catch Weight               74\n",
       "Women's Featherweight      24\n",
       "Name: fight_weightclass, dtype: int64"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check weight class counts\n",
    "df['fight_weightclass'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter by weightclass\n",
    "df = df[df['fight_weightclass'] == 'Lightweight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['date', 'datetime', 'date_formatted']"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for date column\n",
    "date_cols = [col for col in df.columns if 'date' in col]\n",
    "date_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search for Rolling_Kd columns\n",
    "rolling_kd = [col for col in df.columns if 'Rolling_Kd' in col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "unnamed = [n for n in df.columns if 'Unnamed' in n]\n",
    "df.drop(columns=unnamed, inplace=True)\n",
    "#Identify columns with missing values\n",
    "nothere = df.isna().sum()\n",
    "nothere = pd.DataFrame(nothere)\n",
    "nothere = nothere.loc[nothere[0] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get cols with InFightData in the name\n",
    "InFightData = [n for n in df.columns if 'InFightData' in n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop them from df\n",
    "df.drop(columns=InFightData, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop specific categorical columns with too many values, we are using clusters instead\n",
    "df.drop(['A_Affiliation', 'B_Affiliation', 'A_Fighting_Out_Of', 'B_Fighting_Out_Of', 'A_Previous_Promotion', 'B_Previous_Promotion'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "also_drop = ['A_Martial_Art_2', 'B_Martial_Art_2', 'A_Martial_Art_3', 'B_Martial_Art_3',\n",
    "             'A_Martial_Art_1_Category', 'A_Martial_Art_2_Category', 'A_Martial_Art_3_Category',\n",
    "             'B_Martial_Art_1_Category', 'B_Martial_Art_2_Category', 'B_Martial_Art_3_Category']\n",
    "df.drop(columns=also_drop, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all median columns\n",
    "df.drop(columns=[n for n in df.columns if 'median' in n], inplace=True)\n",
    "\n",
    "# drop all std columns\n",
    "df.drop(columns=[n for n in df.columns if 'std' in n], inplace=True)\n",
    "\n",
    "# drop all Std columns\n",
    "df.drop(columns=[n for n in df.columns if 'Std' in n], inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop In-Fight Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop= ['A_Kd', 'B_Kd', 'A_Sig_strike_land',\n",
    "       'A_Sig_strike_att', 'B_Sig_strike_land', 'B_Sig_strike_att',\n",
    "       'A_Sig_strike_percent', 'B_Sig_strike_percent', 'A_Total_Strikes_land',\n",
    "       'A_Total_Strikes_att', 'B_Total_Strikes_land', 'B_Total_Strikes_att',\n",
    "       'A_Total_Strikes_percent', 'B_Total_Strikes_percent',\n",
    "       'A_Takedowns_land', 'A_Takedowns_att', 'B_Takedowns_land',\n",
    "       'B_Takedowns_att', 'A_Takedown_percent', 'B_Takedown_percent',\n",
    "       'A_Sub_Attempts_land', 'A_Sub_Attempts_att', 'B_Sub_Attempts_land',\n",
    "       'B_Sub_Attempts_att', 'A_Rev', 'B_Rev', 'A_Ctrl_time_min',\n",
    "       'A_Ctrl_time_sec', 'B_Ctrl_time_min', 'B_Ctrl_time_sec',\n",
    "       'A_Ctrl_time_tot', 'B_Ctrl_time_tot', 'details','A_Head_Strikes_land',\n",
    "       'A_Head_Strikes_att', 'B_Head_Strikes_land', 'B_Head_Strikes_att',\n",
    "       'A_Head_Strikes_percent', 'B_Head_Strikes_percent',\n",
    "       'A_Body_Strikes_land', 'A_Body_Strikes_att', 'B_Body_Strikes_land',\n",
    "       'B_Body_Strikes_att', 'B_Body_Strikes_percent', 'A_Leg_Strikes_land', 'A_Leg_Strikes_att',\n",
    "       'B_Leg_Strikes_land', 'B_Leg_Strikes_att', 'A_Leg_Strikes_percent',\n",
    "       'B_Leg_Strikes_percent', 'A_Distance_Strikes_land',\n",
    "       'A_Distance_Strikes_att', 'B_Distance_Strikes_land',\n",
    "       'B_Distance_Strikes_att', 'A_Distance_Strikes_percent',\n",
    "       'B_Distance_Strikes_percent', 'A_Clinch_Strikes_land',\n",
    "       'A_Clinch_Strikes_att', 'B_Clinch_Strikes_land', 'B_Clinch_Strikes_att',\n",
    "       'A_Clinch_Strikes_percent', 'B_Clinch_Strikes_percent',\n",
    "       'A_Ground_Strikes_land', 'A_Ground_Strikes_att',\n",
    "       'B_Ground_Strikes_land', 'B_Ground_Strikes_att',\n",
    "       'A_Ground_Strikes_percent', 'B_Ground_Strikes_percent',  'A_Body_Strikes_percent']\n",
    "\n",
    "df.drop(columns=to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all Dif Rows (In-Fight Differentials)\n",
    "dif_rows = ['Dif_Kd', 'Dif_Sig_strike_land', 'Dif_Sig_strike_att', 'Dif_Sig_strike_percent',\n",
    " 'Dif_Total_Strikes_land', 'Dif_Total_Strikes_att', 'Dif_Total_Strikes_percent', 'Dif_Takedowns_land',\n",
    " 'Dif_Takedowns_att', 'Dif_Takedown_percent', 'Dif_Sub_Attempts_land', 'Dif_Sub_Attempts_att',\n",
    " 'Dif_Rev', 'Dif_Ctrl_time_min', 'Dif_Ctrl_time_sec', 'Dif_Ctrl_time_tot', 'Dif_Head_Strikes_land',\n",
    " 'Dif_Head_Strikes_att', 'Dif_Head_Strikes_percent', 'Dif_Body_Strikes_land', 'Dif_Body_Strikes_att',\n",
    " 'Dif_Body_Strikes_percent', 'Dif_Leg_Strikes_land', 'Dif_Leg_Strikes_att', 'Dif_Leg_Strikes_percent',\n",
    " 'Dif_Distance_Strikes_land', 'Dif_Distance_Strikes_att', 'Dif_Distance_Strikes_percent', 'Dif_Clinch_Strikes_land',\n",
    " 'Dif_Clinch_Strikes_att', 'Dif_Clinch_Strikes_percent', 'Dif_Ground_Strikes_land','Dif_Ground_Strikes_att',\n",
    " 'Dif_Ground_Strikes_percent']\n",
    "\n",
    "df.drop(columns=dif_rows, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop instructor columns\n",
    "instructor_cols = [col for col in  df.columns if 'Instructor' in col]\n",
    "\n",
    "df.drop(columns=instructor_cols, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop topdown columns\n",
    "topdown_cols = [col for col in  df.columns if 'topdown' in col]\n",
    "df.drop(columns=topdown_cols, inplace=True)\n",
    "\n",
    "# drop Year Started Columns\n",
    "year_started_cols = [col for col in  df.columns if 'Year_Started' in col]\n",
    "df.drop(columns=year_started_cols, inplace=True)\n",
    "\n",
    "# Drop UFC Wins Total and UFC Losses Totals\n",
    "df.drop(columns=['A_UFC_Wins_Total', 'A_UFC_Losses_Total', 'B_UFC_Wins_Total', 'B_UFC_Losses_Total'], inplace=True)\n",
    "\n",
    "# Drop all Wins_Total and Losses_Total columns\n",
    "df.drop(columns=[col for col in df.columns if 'Wins_Total' in col], inplace=True)\n",
    "df.drop(columns=[col for col in df.columns if 'Losses_Total' in col], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check 'per_round' columns\n",
    "per_round = [n for n in df.columns if 'per_round' in n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check rolling columns\n",
    "rolling = [n for n in df.columns if 'Rolling' in n]\n",
    "rolling_not_per_round = [n for n in rolling if 'per_round' not in n]\n",
    "\n",
    "rolling2 = [n for n in df.columns if 'rolling' in n]\n",
    "rolling2_not_per_round = [n for n in rolling2 if 'per_round' not in n]\n",
    "\n",
    "rolling2_not_percent = [n for n in rolling2_not_per_round if 'percent' not in n]\n",
    "\n",
    "\n",
    "# drop rolling_not_per_round columns\n",
    "df.drop(columns=rolling2_not_percent, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop others\n",
    "to_drop = ['event_title','event_url','date', 'fight_id', 'Fighter_A', 'Fighter_B', 'datetime', 'date_formatted',\n",
    "            'Winner', 'event_code', 'A_Typical_Weightclass', 'B_Typical_Weightclass', 'final_round_seconds' ] \n",
    "\n",
    "df.drop(columns=to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we already have other columns that contain this information\n",
    "to_drop = ['Fighter_A_Odds', 'Fighter_B_Odds', 'Fighter_B_Odds_Change', 'favorite?']\n",
    "df.drop(columns=to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # find Opp_Avg columns\n",
    "# opp_avg = [n for n in df.columns if 'Opp_Avg' in n]\n",
    "# # drop them\n",
    "# df.drop(columns=opp_avg, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get rolling NON per-round cols\n",
    "rolling = [n for n in df.columns if 'Rolling' in n]\n",
    "rolling_not_per_round = [n for n in rolling if 'per_round' not in n]\n",
    "rolling_not_per_round_not_percent = [n for n in rolling_not_per_round if 'percent' not in n]\n",
    "# drop them\n",
    "df.drop(columns=rolling_not_per_round_not_percent, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get percent per round\n",
    "percent_cols = [n for n in df.columns if 'percent' in n]\n",
    "percent_cols_per_round = [n for n in percent_cols if 'per_round' in n]\n",
    "# drop them\n",
    "df.drop(columns=percent_cols_per_round, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop if column has _NA in it\n",
    "df.drop(columns=[n for n in df.columns if '_NA' in n], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop \n",
    "to_drop= ['A_Initial_Martial_Arts_Cats','B_Initial_Martial_Arts_Cats']\n",
    "\n",
    "df.drop(columns=to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix Feature Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_cols = df.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_columns(columns):\n",
    "    new_columns = []\n",
    "    for col in columns:\n",
    "        if \"Rolling_\" in col:\n",
    "            col = col.replace(\"Rolling_\", \"\")\n",
    "        if \"percent_mean\" in col:\n",
    "            col = col.replace(\"percent_mean\", \"Rate\")\n",
    "        if \"mean_per_round\" in col:\n",
    "            col = col.replace(\"mean_per_round\", \"RatePR\")\n",
    "        if \"Opp\" in col:\n",
    "            col = col.replace(\"Opp\", \"Opponent\")\n",
    "        col = col.replace(\"_\", \" \")\n",
    "        new_columns.append(col)\n",
    "    return new_columns\n",
    "\n",
    "# Assuming df is your dataframe\n",
    "df.columns = rename_columns(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_1_1_1_'></a>[Check NAN](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>A Kd RatePR</th>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A Sig strike land RatePR</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A Total Strikes land RatePR</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A Takedowns land RatePR</th>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A Takedowns att RatePR</th>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B Previous Promotion Decision Loss Percent</th>\n",
       "      <td>679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A Fighting Out Of Cluster</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A Days Since Last Fight</th>\n",
       "      <td>284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B Days Since Last Fight</th>\n",
       "      <td>289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Days Since Last Fight Dif</th>\n",
       "      <td>455</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>154 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              0\n",
       "A Kd RatePR                                 145\n",
       "A Sig strike land RatePR                      2\n",
       "A Total Strikes land RatePR                   1\n",
       "A Takedowns land RatePR                      72\n",
       "A Takedowns att RatePR                       32\n",
       "...                                         ...\n",
       "B Previous Promotion Decision Loss Percent  679\n",
       "A Fighting Out Of Cluster                     1\n",
       "A Days Since Last Fight                     284\n",
       "B Days Since Last Fight                     289\n",
       "Days Since Last Fight Dif                   455\n",
       "\n",
       "[154 rows x 1 columns]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check NAN\n",
    "missing = df.isna().sum()\n",
    "missing = pd.DataFrame(missing)\n",
    "missing = missing.loc[missing[0] > 0]\n",
    "missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace NAN with 0\n",
    "df.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace and INF with 0\n",
    "df.replace([np.inf, -np.inf], 0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cols = df.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A Fight in Typical Weightclass', 'B Fight in Typical Weightclass']"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find weightclass columns\n",
    "weightclass = [n for n in all_cols if 'Weightclass' in n]\n",
    "weightclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check Winner column\n",
    "#df['Winner'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.drop(columns=in_fight_cols, inplace=True)\n",
    "#df.drop(columns=in_fight_difs, inplace=True)\n",
    "colz = list(df.columns)\n",
    "# get favorite\n",
    "#df['favorite?'] = np.where(df['Fighter_A_Odds_obf'] < 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    791\n",
       "0    791\n",
       "Name: win?, dtype: int64"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check value counts\n",
    "df['win?'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_1_1_2_'></a>[Assign Target and Split](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_name = \"win?\"\n",
    "y = df[target_name]\n",
    "X = df.drop(columns=[target_name])\n",
    "\n",
    "# Scoring Metric\n",
    "class_metric = 'accuracy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fight weightclass', 'A Martial Art 1', 'B Martial Art 1']"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_columns_selector = selector(dtype_exclude=object)\n",
    "categorical_columns_selector = selector(dtype_include=object)\n",
    "\n",
    "numerical_columns = numerical_columns_selector(X)\n",
    "categorical_columns = categorical_columns_selector(X)\n",
    "\n",
    "categorical_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fight weightclass',\n",
       " 'A Martial Art 1',\n",
       " 'B Martial Art 1',\n",
       " 'A Affiliation Cluster',\n",
       " 'B Affiliation Cluster',\n",
       " 'A Fighting Out Of Cluster',\n",
       " 'B Fighting Out Of Cluster',\n",
       " 'A Previous Promotion Cluster',\n",
       " 'B Previous Promotion Cluster']"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# append affiliaiton columns\n",
    "affiliation_cols = [col for col in X.columns if 'Cluster' in col]\n",
    "categorical_columns.extend(affiliation_cols)\n",
    "categorical_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop affiliation columns from numerical\n",
    "numerical_columns.remove('A Fighting Out Of Cluster')\n",
    "numerical_columns.remove('B Fighting Out Of Cluster')\n",
    "\n",
    "numerical_columns.remove('A Previous Promotion Cluster')\n",
    "numerical_columns.remove('B Previous Promotion Cluster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure categorical_columns are all strings\n",
    "for col in categorical_columns:\n",
    "    X[col] = X[col].astype(str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Affiliation and Fighting_Out_Of columns are both categorical "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_list = X.columns.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_1_1_3_'></a>[Split](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[111, 311, 312, 358, 359, 360, 361, 362, 363]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catcols = []\n",
    "\n",
    "for col in categorical_columns:\n",
    "    ind = col_list.index(col)\n",
    "    catcols.append(ind)\n",
    "\n",
    "catcols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_cols_index = [n for n in range(len(X_train.columns)) if n not in catcols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get the column names from onehotencoder\n",
    "ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore', max_categories=None)\n",
    "checker = ohe.fit_transform(X_train[categorical_columns])\n",
    "\n",
    "# Use get_feature_names_out instead of get_feature_names\n",
    "feature_names_categorical = ohe.get_feature_names_out(input_features=categorical_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorical Feature Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62\n"
     ]
    }
   ],
   "source": [
    "fnc = list(feature_names_categorical)\n",
    "print(len(fnc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "364\n"
     ]
    }
   ],
   "source": [
    "# numerical columns\n",
    "feature_names_numerical = X_train.columns[cont_cols_index]\n",
    "fnn = list(feature_names_numerical)\n",
    "print(len(fnn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_initial_cats = fnn + fnc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_1_2_'></a>[Preprocessing](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>ColumnTransformer(transformers=[(&#x27;num&#x27;,\n",
       "                                 Pipeline(steps=[(&#x27;standardscaler&#x27;,\n",
       "                                                  StandardScaler())]),\n",
       "                                 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,\n",
       "                                  14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24,\n",
       "                                  25, 26, 27, 28, 29, ...]),\n",
       "                                (&#x27;cat&#x27;,\n",
       "                                 Pipeline(steps=[(&#x27;onehotencoder&#x27;,\n",
       "                                                  OneHotEncoder(handle_unknown=&#x27;ignore&#x27;,\n",
       "                                                                sparse_output=False))]),\n",
       "                                 [111, 311, 312, 358, 359, 360, 361, 362,\n",
       "                                  363])])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-13\" type=\"checkbox\" ><label for=\"sk-estimator-id-13\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(transformers=[(&#x27;num&#x27;,\n",
       "                                 Pipeline(steps=[(&#x27;standardscaler&#x27;,\n",
       "                                                  StandardScaler())]),\n",
       "                                 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,\n",
       "                                  14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24,\n",
       "                                  25, 26, 27, 28, 29, ...]),\n",
       "                                (&#x27;cat&#x27;,\n",
       "                                 Pipeline(steps=[(&#x27;onehotencoder&#x27;,\n",
       "                                                  OneHotEncoder(handle_unknown=&#x27;ignore&#x27;,\n",
       "                                                                sparse_output=False))]),\n",
       "                                 [111, 311, 312, 358, 359, 360, 361, 362,\n",
       "                                  363])])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-14\" type=\"checkbox\" ><label for=\"sk-estimator-id-14\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">num</label><div class=\"sk-toggleable__content\"><pre>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 364, 365, 366, 367, 368, 369, 370, 371, 372]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-15\" type=\"checkbox\" ><label for=\"sk-estimator-id-15\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-16\" type=\"checkbox\" ><label for=\"sk-estimator-id-16\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">cat</label><div class=\"sk-toggleable__content\"><pre>[111, 311, 312, 358, 359, 360, 361, 362, 363]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-17\" type=\"checkbox\" ><label for=\"sk-estimator-id-17\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">OneHotEncoder</label><div class=\"sk-toggleable__content\"><pre>OneHotEncoder(handle_unknown=&#x27;ignore&#x27;, sparse_output=False)</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "ColumnTransformer(transformers=[('num',\n",
       "                                 Pipeline(steps=[('standardscaler',\n",
       "                                                  StandardScaler())]),\n",
       "                                 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,\n",
       "                                  14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24,\n",
       "                                  25, 26, 27, 28, 29, ...]),\n",
       "                                ('cat',\n",
       "                                 Pipeline(steps=[('onehotencoder',\n",
       "                                                  OneHotEncoder(handle_unknown='ignore',\n",
       "                                                                sparse_output=False))]),\n",
       "                                 [111, 311, 312, 358, 359, 360, 361, 362,\n",
       "                                  363])])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pipeline for categorical data\n",
    "cat_preprocessing = make_pipeline(\n",
    "    OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False),\n",
    ")\n",
    "# pipeline for numerical data\n",
    "num_preprocessing = make_pipeline(StandardScaler())\n",
    "\n",
    "# combine both pipeline using a columnTransformer\n",
    "preprocessing = ColumnTransformer(\n",
    "    [(\"num\", num_preprocessing, cont_cols_index), (\"cat\", cat_preprocessing, catcols)]\n",
    ")\n",
    "\n",
    "preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_1_3_'></a>[Function Additions](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_result(cv_mean_accuracy, cv_std_accuracy, model_name):\n",
    "    # Define function to save training result\n",
    "\n",
    "    # Access global variable model_summary2\n",
    "    global model_summary2 \n",
    "\n",
    "    # Create a new row with model name, mean accuracy and standard deviation accuracy\n",
    "    row = [(model_name, cv_mean_accuracy, cv_std_accuracy)]\n",
    "\n",
    "    # Create a new dataframe with the row data and column names\n",
    "    dfcols2 = ['Model_Name', 'Cv_Mean_Accuracy', 'Cv_Std_Accuracy']\n",
    "    res = pd.DataFrame(columns = dfcols2, data = row)\n",
    "\n",
    "    # Concatenate the existing model summary dataframe and the new result dataframe\n",
    "    yeep = [model_summary2, res]\n",
    "    model_summary2 = pd.concat(yeep)\n",
    "\n",
    "    # Sort the dataframe by mean accuracy in descending order and drop any duplicates\n",
    "    model_summary2 = model_summary2.sort_values('Cv_Mean_Accuracy', ascending = False)\n",
    "    model_summary2 = model_summary2.drop_duplicates()\n",
    "\n",
    "    # Return the updated model summary dataframe rounded to 3 decimal places\n",
    "    return model_summary2.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(model, model_name):\n",
    "    \"\"\"\n",
    "    # This function runs a machine learning model, produces a confusion matrix, and saves the result\n",
    "    # It takes two parameters: the machine learning model and its name\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Fits the model with the training data\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Obtains the model's predictions using the test data\n",
    "    model_prediction = model.predict(X_test)\n",
    "\n",
    "    # Generates a confusion matrix to evaluate the model's performance\n",
    "    cf_matrix = confusion_matrix(y_test, model_prediction)\n",
    "\n",
    "    # Saves the confusion matrix with the model's name\n",
    "    save_result(cf_matrix, model_name)\n",
    "\n",
    "    # Creates a visualization of the confusion matrix for easy interpretation\n",
    "    cf = make_confusion_matrix(cf_matrix)\n",
    "\n",
    "    # Returns a summary of the model's performance\n",
    "    return model_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfcols2 = ['Model_Name', 'Cv_Mean_Accuracy', 'Cv_Std_Accuracy']\n",
    "# Create a dataframe to store the model summary\n",
    "model_summary2 = pd.DataFrame(columns=dfcols2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fullpipe(preprocessing, model, model_name):\n",
    "    fullpipe = Pipeline(steps=[('preprocess', preprocessing), ('model', model)])\n",
    "    fullpipe.fit(X_train, y_train)\n",
    "    # cross validation\n",
    "    cv = cross_val_score(fullpipe, X_test, y_test, cv=3, scoring='accuracy')\n",
    "    cv_mean = cv.mean()\n",
    "    cv_std = cv.std()\n",
    "    res = save_result(cv_mean, cv_std, model_name)\n",
    "    # pickle model\n",
    "    pickle.dump(fullpipe, open(model_folder + f'{model_name}.pkl', 'wb'))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function cross validates a model and saves the result and a pickle file\n",
    "def score_and_save(model, model_name):\n",
    "    # Cross validate model scores\n",
    "    cv = cross_val_score(model, X_test, y_test, cv=3, scoring='accuracy')\n",
    "    cv_mean = cv.mean()\n",
    "    cv_std = cv.std()\n",
    "    # save result\n",
    "    res = save_result(cv_mean, cv_std, model_name)\n",
    "    # pickle model\n",
    "    pickle.dump(model, open(model_folder + model_name +'.pkl', 'wb'))\n",
    "    # make confusion matrix\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gridsearch_results(grid_clf, param_name,\n",
    "                          num_results=5,\n",
    "                          graph=True,\n",
    "                          display_all_params=False,\n",
    "                          ):\n",
    "    \"\"\"\n",
    "    Visualizes the results of a grid search performed using scikit-learn's GridSearchCV.\n",
    "    Used to perform sensitivity analysis of hyperparameters.\n",
    "\n",
    "    Parameters:\n",
    "    - grid_clf: GridSearchCV object (fitted)\n",
    "    - param_name: str, the name of the hyperparameter to be plotted\n",
    "    - num_results: int, number of top results to display in tabular form (default: 15)\n",
    "    - graph: bool, whether to display a plot (default: True)\n",
    "    - display_all_params: bool, whether to display all hyperparameters of the best estimator (default: True)\n",
    "    \"\"\"\n",
    "    \n",
    "    if not isinstance(grid_clf, GridSearchCV):\n",
    "        raise ValueError(\"grid_clf must be an instance of GridSearchCV.\")\n",
    "    \n",
    "    if not hasattr(grid_clf, 'cv_results_'):\n",
    "        raise ValueError(\"GridSearchCV object must be fitted with data.\")\n",
    "    \n",
    "    clf = grid_clf.best_estimator_\n",
    "    clf_params = grid_clf.best_params_\n",
    "    clf_score = grid_clf.best_score_\n",
    "    clf_stdev = grid_clf.cv_results_['std_test_score'][grid_clf.best_index_]\n",
    "    cv_results = grid_clf.cv_results_\n",
    "\n",
    "    if param_name not in grid_clf.param_grid:\n",
    "        raise ValueError(f\"{param_name} not found in hyperparameters used in the grid search.\")\n",
    "\n",
    "    print(f\"best parameters: {clf_params}\")\n",
    "    print(f\"best score:      {clf_score:0.5f} (+/-{clf_stdev:0.5f})\")\n",
    "    \n",
    "    if display_all_params:\n",
    "        pprint.pprint(clf.get_params())\n",
    "\n",
    "    # pick out the best results\n",
    "    scores_df = pd.DataFrame(cv_results).sort_values(by='rank_test_score')\n",
    "\n",
    "    new_param = 'param_' + param_name\n",
    "    cv_results[new_param] = pd.Series(cv_results[new_param]).replace({np.log2(np.e): 'log2', np.sqrt(np.e): 'sqrt'})\n",
    "\n",
    "    best_row = scores_df.iloc[0, :]\n",
    "    best_mean = best_row['mean_test_score']\n",
    "    best_stdev = best_row['std_test_score']\n",
    "    best_param = best_row[new_param]\n",
    "\n",
    "    # display the top 'num_results' results\n",
    "    top_results =pd.DataFrame(cv_results).sort_values(by='rank_test_score').head(num_results)\n",
    "\n",
    "    # PLOT\n",
    "    # Convert the hyperparameter values to strings\n",
    "    scores_df[new_param] = scores_df[new_param].astype(str)\n",
    "    scores_df = scores_df.sort_values(by=new_param)\n",
    "\n",
    "    means = scores_df['mean_test_score']\n",
    "    stds = scores_df['std_test_score']\n",
    "    params = scores_df[new_param]\n",
    "\n",
    "    # Get the index of the best hyperparameter value\n",
    "    best_param = str(best_param)  # Ensure best_param is a string for comparison\n",
    "    best_param_index = scores_df[new_param].eq(best_param).idxmax()\n",
    "\n",
    "\n",
    "    # plot\n",
    "    if graph:\n",
    "        fig, ax = plt.subplots(figsize=(8, 4))\n",
    "        try:\n",
    "            ax.errorbar(range(len(params)), means, yerr=stds)  # Use index instead of actual values\n",
    "            ax.set_xticks(range(len(params)))  # Set x-axis labels to the hyperparameter values\n",
    "            ax.set_xticklabels(params, rotation=45, ha='right')  # every other x-tick\n",
    "            \n",
    "            ax.axhline(y=best_mean + best_stdev, color='red')\n",
    "            ax.axhline(y=best_mean - best_stdev, color='red')\n",
    "            ax.plot(best_param_index, best_mean, 'or')  # Use the index of the best_param\n",
    "            ax.set_title(f\"{param_name} vs Score\\nBest Score {clf_score:0.5f}\")\n",
    "            ax.set_xlabel(param_name)\n",
    "            ax.set_ylabel('Score')\n",
    "            # make y-axis ticks every 5 ticks\n",
    "            ax.yaxis.set_major_locator(ticker.MultipleLocator(0.25))\n",
    "            plt.show()\n",
    "        except TypeError:\n",
    "            ax.plot(best_param_index, best_mean, 'or')  # Use the index of the best_param\n",
    "            ax.set_title(f\"{param_name} vs Score\\nBest Score {clf_score:0.5f}\")\n",
    "            ax.set_xlabel(param_name)\n",
    "            ax.set_ylabel('Score')\n",
    "            plt.show()\n",
    "\n",
    "    return top_results, fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc2_'></a>[Run Vanilla Decision Tree](#toc0_)\n",
    "Decion trees are good places to start when building a simple model to improve on. We might be able to learn something from it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_1_1_'></a>[Run Vanilla Models](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_fullpipe(preprocessing, LogisticRegression(), 'Logistic_Regression')\n",
    "create_fullpipe(preprocessing, RandomForestClassifier(), 'Random_Forest')\n",
    "create_fullpipe(preprocessing, DecisionTreeClassifier(), 'Decision_Tree')\n",
    "create_fullpipe(preprocessing, BaggingClassifier(), 'Bagged_Trees')\n",
    "create_fullpipe(preprocessing, ExtraTreesClassifier(), 'Extra_Trees')\n",
    "create_fullpipe(preprocessing, KNeighborsClassifier(), 'K_Neighbors')\n",
    "create_fullpipe(preprocessing, XGBClassifier(eval_metric = 'logloss'), 'XGBoost')\n",
    "create_fullpipe(preprocessing, GradientBoostingClassifier(), 'Gradient_Boosting')\n",
    "create_fullpipe(preprocessing, LinearDiscriminantAnalysis(), 'LDA')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple Decision Tree\n",
    "\n",
    "First I want to see a simple decision tree, optimized with GridSearchCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load decision tree classifier from pickle\n",
    "decision_tree = pickle.load(open(model_folder+'Decision_Tree.pkl', 'rb'))\n",
    "\n",
    "# check if Decision Tree Grid Search already exists\n",
    "if os.path.exists(model_folder + 'Decision_Tree_Gridsearched.pkl'):\n",
    "    # print the path + exists\n",
    "    print(model_folder + 'Decision_Tree_Gridsearched.pkl exists')\n",
    "    # load decision tree grid search from pickle\n",
    "    decision_tree = pickle.load(open(model_folder + 'Decision_Tree_Gridsearched.pkl', 'rb'))\n",
    "    # run through test and save\n",
    "    final_decision_tree =score_and_save(decision_tree, 'Decision_Tree_Gridsearched')\n",
    "    best_pipeline = decision_tree\n",
    "    print(final_decision_tree)\n",
    "\n",
    "\n",
    "else:\n",
    "    # print the path + does not exist\n",
    "    print(model_folder + 'Decision_Tree_Gridsearched.pkl does not exist')\n",
    "    # Create a dictionary of hyperparameters to search\n",
    "    param_grid = {\n",
    "        \"model__max_depth\": [None, 3, 5, 7, 9, 11, 13, 15, 17, 19, 50],\n",
    "        \"model__min_samples_split\": [.01, .05, .1, .2, .001],\n",
    "        \"model__min_samples_leaf\": [.01, .05, .1, .2, .001],\n",
    "        \"model__criterion\": [\"gini\" , \"entropy\"],\n",
    "        \"model__max_features\": [None, \"auto\", \"sqrt\", \"log2\", .6, .7, .8, .9],\n",
    "    }\n",
    "\n",
    "    # Create a gridsearch of the decision tree, using 3-fold cross validation,\n",
    "    # search across the specified hyperparameters\n",
    "    # Use all available cores\n",
    "    grid_search = GridSearchCV(\n",
    "        decision_tree, param_grid, cv=3, scoring=\"accuracy\", verbose=3\n",
    "    )\n",
    "\n",
    "    # Fit the gridsearch to the training data\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Print the best parameters and best score found by the gridsearch\n",
    "    best_parameters = grid_search.best_params_\n",
    "\n",
    "    # get the whole gridsearch results\n",
    "    gs_results = grid_search.cv_results_\n",
    "    gs_results_df = pd.DataFrame(grid_search.cv_results_)\n",
    "    gs_results_df.head()\n",
    "    best_estimator = grid_search.best_estimator_\n",
    "    best_pipeline = best_estimator\n",
    "\n",
    "    final_decision_tree = score_and_save(best_estimator, 'Decision_Tree_Gridsearched')\n",
    "    print(final_decision_tree)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_decision_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Access the decision tree model from the pipeline\n",
    "decision_tree_model = best_pipeline.named_steps['model']\n",
    "\n",
    "# Get the numerical column names\n",
    "numerical_feature_names = numerical_columns\n",
    "\n",
    "# Get the one-hot-encoded categorical column names\n",
    "categorical_feature_names = feature_names_categorical\n",
    "\n",
    "# Combine numerical and one-hot-encoded categorical column names\n",
    "feature_names = numerical_feature_names + list(categorical_feature_names)\n",
    "\n",
    "# Get the unique class names (sorted) and convert them to strings\n",
    "class_names = sorted(y.unique().astype(str))  # Convert class names to strings\n",
    "\n",
    "# Visualize the decision tree with proper labels\n",
    "plt.figure(figsize=(15, 10), dpi=1000)\n",
    "plot_tree(decision_tree_model, feature_names=feature_names, class_names=class_names, filled=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# viz the distribution of head strikes rate\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "sns.distplot(df['Dif Odds'], ax=ax)\n",
    "ax.set_title('Distribution of A Head Strikes Rate')\n",
    "ax.set_xlabel('A Head Strikes Rate')\n",
    "ax.set_ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize with Graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PATH\"] += '/Users/travisroyce/anaconda3/envs/Trav_311/lib/python3.11/site-packages/graphviz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Access the decision tree model from the pipeline\n",
    "# decision_tree_model = best_pipeline.named_steps['model']\n",
    "\n",
    "# # Get the numerical column names\n",
    "# numerical_feature_names = numerical_columns\n",
    "\n",
    "# # Get the one-hot-encoded categorical column names\n",
    "# categorical_feature_names = feature_names_categorical\n",
    "\n",
    "# # Combine numerical and one-hot-encoded categorical column names\n",
    "# feature_names = numerical_feature_names + list(categorical_feature_names)\n",
    "\n",
    "# # Get the unique class names (sorted) and convert them to strings\n",
    "# class_names = sorted(y.unique().astype(str))\n",
    "\n",
    "# # Export the decision tree to Graphviz format\n",
    "# dot_data = export_graphviz(decision_tree_model, feature_names=feature_names,\n",
    "#                            class_names=class_names, filled=True)\n",
    "\n",
    "# # Add size attribute to the dot_data string (width, height) in inches\n",
    "# dot_data = dot_data.replace(\"digraph Tree {\", 'digraph Tree { size=\"15,10\";')\n",
    "\n",
    "# graph = graphviz.Source(dot_data)\n",
    "# graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Access the decision tree model from the pipeline\n",
    "# decision_tree_model = best_pipeline.named_steps['model']\n",
    "\n",
    "# # Get the numerical column names\n",
    "# numerical_feature_names = numerical_columns\n",
    "\n",
    "# # Get the one-hot-encoded categorical column names\n",
    "# categorical_feature_names = feature_names_categorical\n",
    "\n",
    "# # Combine numerical and one-hot-encoded categorical column names\n",
    "# feature_names = numerical_feature_names + list(categorical_feature_names)\n",
    "\n",
    "# # Get the unique class names (sorted) and convert them to strings\n",
    "# class_names = sorted(y.unique().astype(str))\n",
    "\n",
    "# def explain_tree(decision_tree, feature_names, class_names, node=0, depth=0, explanation=[]):\n",
    "#     if decision_tree.tree_.children_left[node] == _tree.TREE_LEAF:\n",
    "#         # This is a leaf node, get the predicted class\n",
    "#         class_index = decision_tree.tree_.value[node].argmax()\n",
    "#         class_name = class_names[class_index]\n",
    "#         # Construct the explanation for the leaf node\n",
    "#         explanation.append(f'Then this observation is classified as \"{class_name}\".')\n",
    "#         return '\\n'.join(explanation)\n",
    "#     else:\n",
    "#         # This is an internal node, get the feature and threshold for the decision rule\n",
    "#         feature = feature_names[decision_tree.tree_.feature[node]]\n",
    "#         threshold = decision_tree.tree_.threshold[node]\n",
    "#         # Construct the explanation for the decision rule\n",
    "#         if depth == 0:\n",
    "#             explanation.append(f'If the {feature} <= {threshold:.2f},')\n",
    "#         else:\n",
    "#             explanation.append(f'and if the {feature} <= {threshold:.2f},')\n",
    "#         # Recur on the left subtree (True branch)\n",
    "#         left_explanation = explain_tree(decision_tree, feature_names, class_names, decision_tree.tree_.children_left[node], depth+1, explanation.copy())\n",
    "#         # Update the explanation for the right branch\n",
    "#         explanation[-1] = explanation[-1].replace('<=', '>')\n",
    "#         # Recur on the right subtree (False branch)\n",
    "#         right_explanation = explain_tree(decision_tree, feature_names, class_names, decision_tree.tree_.children_right[node], depth+1, explanation.copy())\n",
    "#         return f'{left_explanation}\\n\\nOtherwise, {right_explanation}'\n",
    "    \n",
    "\n",
    "# # Use the recursive function to generate the explanation\n",
    "# explanation = explain_tree(decision_tree_model, feature_names, class_names)\n",
    "# print(explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_tree(decision_tree, feature_names, class_names, node=0, depth=0, explanation=[]):\n",
    "    samples = decision_tree.tree_.n_node_samples[node]\n",
    "    values = decision_tree.tree_.value[node][0]\n",
    "    value_proportions = values / sum(values)\n",
    "    gini_impurity = decision_tree.tree_.impurity[node]\n",
    "\n",
    "    if decision_tree.tree_.children_left[node] == _tree.TREE_LEAF:\n",
    "        class_index = decision_tree.tree_.value[node].argmax()\n",
    "        class_name = class_names[class_index]\n",
    "        explanation.append(f'Then this observation is classified as \"{class_name}\".')\n",
    "        return '\\n'.join(explanation)\n",
    "    else:\n",
    "        feature = feature_names[decision_tree.tree_.feature[node]]\n",
    "        threshold = decision_tree.tree_.threshold[node]\n",
    "\n",
    "        if depth == 0:\n",
    "            explanation.append(f'If the {feature} <= {threshold:.2f},')\n",
    "        else:\n",
    "            explanation.append(f'and if the {feature} <= {threshold:.2f},')\n",
    "\n",
    "        explanation.append(f'This node has {samples} samples, with class proportions: {\", \".join([f\"{class_names[i]}: {value_proportions[i]:.2f}\" for i in range(len(values))])}.')\n",
    "        explanation.append(f'The Gini impurity for this node is {gini_impurity:.2f}.')\n",
    "\n",
    "        left_explanation = explain_tree(decision_tree, feature_names, class_names, decision_tree.tree_.children_left[node], depth+1, explanation.copy())\n",
    "\n",
    "        explanation[-3] = explanation[-3].replace('<=', '>')\n",
    "        right_explanation = explain_tree(decision_tree, feature_names, class_names, decision_tree.tree_.children_right[node], depth+1, explanation.copy())\n",
    "\n",
    "        return f'{left_explanation}\\n\\nOtherwise, {right_explanation}'\n",
    "\n",
    "explanation = explain_tree(decision_tree_model, feature_names, class_names)\n",
    "print(explanation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import graphviz\n",
    "# from sklearn.tree import export_graphviz\n",
    "# from IPython.display import display\n",
    "\n",
    "# def explain_tree(decision_tree, feature_names, class_names, node=0, depth=0, explanation=[]):\n",
    "#     if decision_tree.tree_.children_left[node] == _tree.TREE_LEAF:\n",
    "#         class_index = decision_tree.tree_.value[node].argmax()\n",
    "#         class_name = class_names[class_index]\n",
    "#         explanation.append(f'Then this observation is classified as \"{class_name}\".')\n",
    "#         return explanation\n",
    "#     else:\n",
    "#         feature = feature_names[decision_tree.tree_.feature[node]]\n",
    "#         threshold = decision_tree.tree_.threshold[node]\n",
    "#         if depth == 0:\n",
    "#             explanation.append(f'If the {feature} <= {threshold:.2f},')\n",
    "#         else:\n",
    "#             explanation.append(f'and if the {feature} <= {threshold:.2f},')\n",
    "#         left_explanation = explain_tree(decision_tree, feature_names, class_names, decision_tree.tree_.children_left[node], depth+1, explanation.copy())\n",
    "#         explanation[-1] = explanation[-1].replace('<=', '>')\n",
    "#         right_explanation = explain_tree(decision_tree, feature_names, class_names, decision_tree.tree_.children_right[node], depth+1, explanation.copy())\n",
    "#         return left_explanation + right_explanation\n",
    "\n",
    "# node_explanations = explain_tree(decision_tree_model, feature_names, class_names)\n",
    "\n",
    "# def my_node_label(node):\n",
    "#     return node_explanations[node]\n",
    "\n",
    "# dot_data = export_graphviz(decision_tree_model, out_file=None, feature_names=feature_names, class_names=class_names, filled=True, rounded=True, special_characters=True, node_ids=True, label=my_node_label)\n",
    "# graph = graphviz.Source(dot_data)\n",
    "# display(graph)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import graphviz\n",
    "# from sklearn.tree import _tree\n",
    "\n",
    "# def build_graph(decision_tree, feature_names, class_names, title, label_0='No', label_1='Yes', node=0, depth=0, parent=None, branch=None):\n",
    "#     samples = decision_tree.tree_.n_node_samples[node]\n",
    "#     values = decision_tree.tree_.value[node][0]\n",
    "#     value_proportions = values / sum(values)\n",
    "#     gini_impurity = decision_tree.tree_.impurity[node]\n",
    "\n",
    "#     if decision_tree.tree_.children_left[node] == _tree.TREE_LEAF:\n",
    "#         class_index = decision_tree.tree_.value[node].argmax()\n",
    "#         class_name = class_names[class_index]\n",
    "#         explanation = f'Then this observation is classified as \"{class_name}\".'\n",
    "#     else:\n",
    "#         feature = feature_names[decision_tree.tree_.feature[node]]\n",
    "#         threshold = decision_tree.tree_.threshold[node]\n",
    "#         explanation = f'If the {feature} <= {threshold:.2f},' if depth == 0 else f'and if the {feature} <= {threshold:.2f},'\n",
    "\n",
    "#     explanation += (f'\\n{label_0}: {value_proportions[0]*100:.0f}% | {label_1}: {value_proportions[1]*100:.0f}%'\n",
    "#                     f'\\n({samples} Samples, Gini = {gini_impurity:.2f})')\n",
    "#     graph.node(str(node), label=explanation, shape='rectangle', fontsize='10')\n",
    "\n",
    "#     if parent is not None:\n",
    "#         graph.edge(str(parent), str(node), label=branch)\n",
    "\n",
    "#     if decision_tree.tree_.children_left[node] != _tree.TREE_LEAF:\n",
    "#         build_graph(decision_tree, feature_names, class_names, title, label_0, label_1, decision_tree.tree_.children_left[node], depth+1, node, \"True\")\n",
    "#         build_graph(decision_tree, feature_names, class_names, title, label_0, label_1, decision_tree.tree_.children_right[node], depth+1, node, \"False\")\n",
    "\n",
    "# # Create a new Graphviz graph\n",
    "# graph = graphviz.Digraph(format='png', graph_attr={'splines': 'ortho', 'ranksep': '0.5'})\n",
    "\n",
    "# # Specify custom labels for class 0 and class 1\n",
    "# custom_label_0 = 'Negative'\n",
    "# custom_label_1 = 'Positive'\n",
    "\n",
    "# # Specify the title for the graph\n",
    "# graph_title = 'Decision Tree Visualization: Will Red Fighter Win?'\n",
    "\n",
    "# # Set the title for the graph\n",
    "# graph.attr(label=graph_title, labelloc='t', fontsize='16')\n",
    "\n",
    "# # Call the build_graph function to construct the graph recursively\n",
    "# build_graph(decision_tree_model, feature_names, class_names, graph_title, custom_label_0, custom_label_1)\n",
    "\n",
    "# # Display the graph once it has been fully constructed\n",
    "# display(graph)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Sequential GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load random forest model from pickle\n",
    "\n",
    "with open(model_folder + 'Random_Forest.pkl', 'rb') as f:\n",
    "    rf_model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def random_forest_sequential_gridsearch(model, X, y):\n",
    "\n",
    "    # Define the grid of hyperparameters\n",
    "    param_grids = [\n",
    "        {'max_depth': [None],\n",
    "         'max_features': ['auto', 'sqrt'],\n",
    "         'criterion': ['gini'],\n",
    "         'min_samples_split': list(range(2, 15)),\n",
    "         'min_samples_leaf': list(range(1, 15)),\n",
    "         'bootstrap': [True],\n",
    "         'max_leaf_nodes': [None] + list(range(2, 6)),\n",
    "         'min_impurity_decrease': [x / 10 for x in range(4)],\n",
    "         'n_estimators': [100, 500],\n",
    "         'n_jobs':[16]}\n",
    "    ]\n",
    "\n",
    "    best_params = {}\n",
    "\n",
    "    counter = 0\n",
    "\n",
    "    # Perform a sequential grid search, updating the best parameters found for each hyperparameter\n",
    "    for param_name, param_values in param_grids[0].items():\n",
    "        # if the file exists, load it and skip the gridsearch, and test it\n",
    "        if os.path.isfile(model_folder + 'Random_Forest_Sequential_Gridsearch_part_{counter}.pkl'):\n",
    "            with open(model_folder + 'Random_Forest_Sequential_Gridsearch_part_{counter}.pkl', 'rb') as f:\n",
    "                best_estimator = pickle.load(f)\n",
    "            score_and_save(best_estimator, f'Random_Forest_Sequential_Gridsearch_part_{counter}')\n",
    "            counter += 1\n",
    "            continue\n",
    "\n",
    "        else:\n",
    "\n",
    "            # Use the list of values for the current hyperparameter\n",
    "            current_grid = {param_name: param_values}\n",
    "            # Update the best parameters found so far\n",
    "            for k, v in best_params.items():\n",
    "                if k != param_name:\n",
    "                    current_grid[k] = [v]  # Wrap individual values in a list\n",
    "            current_grid = {'model__' + k: v for k, v in current_grid.items()}\n",
    "\n",
    "            # Use GridSearchCV with the current hyperparameter grid\n",
    "            gridsearch = GridSearchCV(model, current_grid, cv=4, scoring='accuracy', verbose=3)\n",
    "            gridsearch.fit(X_train, y_train)\n",
    "            best_estimator = gridsearch.best_estimator_\n",
    "            score_and_save(best_estimator, f'Random_Forest_Sequential_Gridsearch_part_{counter}')\n",
    "\n",
    "            best_param_value = gridsearch.best_params_['model__' + param_name]\n",
    "\n",
    "            # Update the best parameters dictionary with the best value found for the current hyperparameter\n",
    "            best_params[param_name] = best_param_value\n",
    "            print(f'Best {param_name}: {best_param_value}')\n",
    "            counter += 1\n",
    "        \n",
    "\n",
    "    return best_estimator, best_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the gridsearch\n",
    "best_estimator, best_params = random_forest_sequential_gridsearch(rf_model, X_train, y_train)\n",
    "\n",
    "final_gridsearched_model = score_and_save(best_estimator, 'Random_Forest_Sequential_Gridsearch')\n",
    "final_gridsearched_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the model exists already in models folder\n",
    "def check_model(model_name):\n",
    "    if os.path.isfile(model_folder + model_name +'.pkl'):\n",
    "        print('Model already exists')\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "if check_model('Random_Forest_1000') == False:\n",
    "    create_fullpipe(preprocessing, RandomForestClassifier(n_estimators=1000), 'Random_Forest_1000')\n",
    "else:\n",
    "    # load model from pickle\n",
    "    with open(model_folder + 'Random_Forest_1000.pkl', 'rb') as f:\n",
    "        rf_1000_model = pickle.load(f)\n",
    "    # test it\n",
    "    score_and_save(rf_1000_model, 'Random_Forest_1000')\n",
    "\n",
    "if check_model('Random_Forest_500') == False:\n",
    "    create_fullpipe(preprocessing, RandomForestClassifier(n_estimators=500), 'Random_Forest_500')\n",
    "    # load model from pickle\n",
    "    with open(model_folder + 'Random_Forest_500.pkl', 'rb') as f:\n",
    "        rf_500_model = pickle.load(f)\n",
    "    # test it\n",
    "    score_and_save(rf_500_model, 'Random_Forest_500')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load random forest model from pickle\n",
    "\n",
    "with open(model_folder + 'Random_Forest.pkl', 'rb') as f:\n",
    "    rf_model = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_1_2_'></a>[Model 1: XGBoost](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_boost = pickle.load(open(model_folder + 'XGBoost.pkl', 'rb'))\n",
    "keys = xg_boost.get_params().keys()\n",
    "model_keys = [key for key in keys if 'model' in key]\n",
    "xg_boost[1].get_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def xgboost_sequential_gridsearch(model, X, y):\n",
    "\n",
    "    # Define the grid of hyperparameters\n",
    "    param_grids = [\n",
    "        {'model__max_depth': [None],\n",
    "         'model__learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "         'model__min_child_weight': list(range(1, 12)),\n",
    "         'model__gamma': [0, 0.1, 0.15, 0.2, 0.3],\n",
    "         'model__subsample': [0.7, 0.8, 1.0],\n",
    "         'model__colsample_bytree': [0.7, 0.8, 1.0],\n",
    "         'model__n_estimators': [100, 500]}\n",
    "    ]\n",
    "\n",
    "    best_params = {}\n",
    "\n",
    "    counter = 0\n",
    "\n",
    "    # Perform a sequential grid search, updating the best parameters found for each hyperparameter\n",
    "    for param_name, param_values in param_grids[0].items():\n",
    "\n",
    "        # If the file exists, load it and skip the gridsearch, and test it\n",
    "        if os.path.isfile(model_folder + 'XGBoost_Sequential_Gridsearch_part_{counter}.pkl'):\n",
    "            with open(model_folder + 'XGBoost_Sequential_Gridsearch_part_{counter}.pkl', 'rb') as f:\n",
    "                best_estimator = pickle.load(f)\n",
    "            score_and_save(best_estimator, 'XGBoost_Sequential_Gridsearch_part_{counter}')\n",
    "            counter += 1\n",
    "            continue\n",
    "\n",
    "        else:\n",
    "            # Use the list of values for the current hyperparameter\n",
    "            current_grid = {param_name: param_values}\n",
    "            # Update the best parameters found so far\n",
    "            for k, v in best_params.items():\n",
    "                if k != param_name:\n",
    "                    current_grid[k] = [v]  # Wrap individual values in a list\n",
    "            current_grid = {k: v for k, v in current_grid.items()}\n",
    "\n",
    "            # Use GridSearchCV with the current hyperparameter grid\n",
    "            gridsearch = GridSearchCV(model, current_grid, cv=4, scoring='accuracy', verbose=3)\n",
    "            gridsearch.fit(X, y)\n",
    "            best_estimator = gridsearch.best_estimator_\n",
    "            score_and_save(best_estimator, f'XGBoost_Sequential_Gridsearch_part_{counter}')\n",
    "\n",
    "            best_param_value = gridsearch.best_params_[param_name]\n",
    "\n",
    "            # Update the best parameters dictionary with the best value found for the current hyperparameter\n",
    "            best_params[param_name] = best_param_value\n",
    "            print(f'Best {param_name}: {best_param_value}')\n",
    "            counter += 1\n",
    "\n",
    "    return best_estimator, best_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the gridsearch\n",
    "best_estimator, best_params = xgboost_sequential_gridsearch(xg_boost, X_train, y_train)\n",
    "\n",
    "final_gridsearched_model = score_and_save(best_estimator, 'XGBoost_Sequential_Gridsearch')\n",
    "final_gridsearched_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_1_3_'></a>[Extra Trees](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_trees = pickle.load(open(model_folder + 'Extra_Trees.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = extra_trees.get_params().keys()\n",
    "model_keys = [key for key in keys if 'model' in key]\n",
    "model_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extra_trees_sequential_gridsearch(model, X, y):\n",
    "    # Define the grid of hyperparameters\n",
    "    param_grids = [\n",
    "        {\n",
    "         'model__max_depth': [None] + list(range(2, 101, 2)),\n",
    "         'model__max_features': ['auto', 'sqrt'],\n",
    "         'model__criterion': ['gini', 'entropy'],\n",
    "         'model__min_samples_split': list(range(2, 12)),\n",
    "         'model__min_samples_leaf': list(range(1, 12)),\n",
    "         'model__bootstrap': [True, False],\n",
    "         'model__max_leaf_nodes': [None] + list(range(2, 6)),\n",
    "         'model__min_impurity_decrease': [.01, .05, .1, .15, .2],\n",
    "         'model__n_jobs': [16],\n",
    "         'model__n_estimators': [100, 500]}\n",
    "    ]\n",
    "\n",
    "    best_params = {}\n",
    "    counter = 0\n",
    "\n",
    "    # Perform a sequential grid search, updating the best parameters found for each hyperparameter\n",
    "    for param_name, param_values in param_grids[0].items():\n",
    "        # if the file exists, load it and skip the gridsearch, and test it\n",
    "        if os.path.isfile(model_folder + 'Extra_Trees_Sequential_Gridsearch_part_{counter}.pkl'):\n",
    "            with open(model_folder + 'Extra_Trees_Sequential_Gridsearch_part_{counter}.pkl', 'rb') as f:\n",
    "                best_estimator = pickle.load(f)\n",
    "            score_and_save(best_estimator, f'Extra_Trees_Sequential_Gridsearch_part_{counter}')\n",
    "            counter += 1\n",
    "            continue\n",
    "\n",
    "        else:\n",
    "\n",
    "            # Use the list of values for the current hyperparameter\n",
    "            current_grid = {param_name: param_values}\n",
    "            # Update the best parameters found so far\n",
    "            for k, v in best_params.items():\n",
    "                if k != param_name:\n",
    "                    current_grid[k] = [v]  # Wrap individual values in a list\n",
    "            current_grid = {k: v for k, v in current_grid.items()}\n",
    "\n",
    "            # Use GridSearchCV with the current hyperparameter grid\n",
    "            gridsearch = GridSearchCV(model, current_grid, cv=4, scoring='accuracy', verbose=3)\n",
    "            gridsearch.fit(X_train, y_train)\n",
    "            best_estimator = gridsearch.best_estimator_\n",
    "            score_and_save(best_estimator, f'Extra_Trees_Sequential_Gridsearch_part_{counter}')\n",
    "\n",
    "            best_param_value = gridsearch.best_params_[param_name]\n",
    "\n",
    "            # Update the best parameters dictionary with the best value found for the current hyperparameter\n",
    "            best_params[param_name] = best_param_value\n",
    "            print(f'Best {param_name}: {best_param_value}')\n",
    "            counter += 1\n",
    "\n",
    "    return best_estimator, best_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the grid search\n",
    "best_estimator, best_params = extra_trees_sequential_gridsearch(extra_trees, X_train, y_train)\n",
    "\n",
    "final_gridsearched_model = score_and_save(best_estimator, 'Extra_Trees_Sequential_Gridsearch')\n",
    "final_gridsearched_model.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc2_1_3_1_'></a>[Best Model](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick the top model in saved models\n",
    "top_model = model_summary2.iloc[0, 0]\n",
    "top_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = pickle.load(open(model_folder + ''+top_model+'.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model['model'].get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_list= list(feature_names_categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cats = cat_list + numerical_columns\n",
    "print(len(all_cats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what values are in all_cats that are not in all_initial_cats?\n",
    "set(all_cats) - set(all_initial_cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_imp = best_model.steps[1][1].feature_importances_\n",
    "f_imp_df = pd.DataFrame(f_imp, index = all_cats, columns = ['Importance'])\n",
    "f_imp_df = f_imp_df.sort_values('Importance', ascending = False)\n",
    "best_f_imp_df = f_imp_df.round(3)\n",
    "best_f_imp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_f_imp_df[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_f_imp_df[50:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model['model'].get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Model (Usually) Important Features\n",
    "\n",
    "The last model is typically the best model. Lets look at the most important features. \n",
    "\n",
    "We can see Afiliation is important, so we can make remove it from the list as the categorical nature of it takes too many items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all rows with \"Affiliation\" in it\n",
    "best_f_imp_df = best_f_imp_df[~best_f_imp_df.index.str.contains('Affiliation')]\n",
    "# drop all rows with \"Martial_Art\" in it\n",
    "best_f_imp_df = best_f_imp_df[~best_f_imp_df.index.str.contains('Martial_Art')]\n",
    "best_f_imp_df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stophere"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost Randomized Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import uniform, randint\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fullpipe_v2(preprocessing, base_model, model_name, param_distributions, n_iter=100):\n",
    "    # Create a full pipeline with preprocessing and the base model\n",
    "    fullpipe = Pipeline(steps=[('preprocess', preprocessing), ('model', base_model)])\n",
    "\n",
    "    # Setup randomized search\n",
    "    random_search = RandomizedSearchCV(\n",
    "        fullpipe, param_distributions=param_distributions, n_iter=n_iter, \n",
    "        scoring='accuracy', cv=3, verbose=3, random_state=42, n_jobs=-1)\n",
    "\n",
    "    # Fit model with randomized search\n",
    "    random_search.fit(X_train, y_train)\n",
    "\n",
    "    # Best pipeline after randomized search\n",
    "    best_pipeline = random_search.best_estimator_\n",
    "\n",
    "    # Evaluate on test data\n",
    "    cv = cross_val_score(best_pipeline, X_test, y_test, cv=3, scoring='accuracy')\n",
    "    cv_mean = cv.mean()\n",
    "    cv_std = cv.std()\n",
    "\n",
    "    # save result\n",
    "    res = save_result(cv_mean, cv_std, model_name)\n",
    "\n",
    "    # pickle the best model\n",
    "    pickle.dump(best_pipeline, open(model_folder + f'{model_name}.pkl', 'wb'))\n",
    "    \n",
    "    return res, best_pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_distributions = {\n",
    "    'model__n_estimators': randint(50, 200),\n",
    "    'model__learning_rate': uniform(0.01, 0.2),\n",
    "    'model__max_depth': [None, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50],\n",
    "    'model__colsample_bytree': uniform(0.7, 0.3),\n",
    "    'model__subsample': uniform(0.7, 0.3)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = xgb.XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\")\n",
    "model_name = \"xgboost_random_search_2\"\n",
    "\n",
    "result, best_pipeline = create_fullpipe_v2(preprocessing, base_model, model_name, param_distributions, n_iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = xgb.XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\")\n",
    "model_name = \"xgboost_random_search_3\"\n",
    "\n",
    "result, best_pipeline = create_fullpipe_v2(preprocessing, base_model, model_name, param_distributions, n_iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Trees Randomized Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_distributions = {\n",
    "    'model__n_estimators': [100, 1000],\n",
    "    'model__max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'model__max_depth': [None, 4, 5, 6, 7, 8, 9, 10, 12, 15, 18, 20, 30],\n",
    "    'model__min_samples_split': [2, 4, 5, 7, 8, 7, 10, 12, 15],\n",
    "    'model__min_samples_leaf': [1, 2, 4, 6, 8, 13, 15],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run extra trees gridsearch\n",
    "base_model = ExtraTreesClassifier()\n",
    "model_name = \"extra_trees_Random_Search_2\"\n",
    "\n",
    "result, best_pipeline = create_fullpipe_v2(preprocessing, base_model, model_name, param_distributions, n_iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_distributions = {\n",
    "    'model__n_estimators': [50, 100, 200],\n",
    "    'model__learning_rate': [0.001, 0.01, 0.1, 0.5, 1],\n",
    "    'model__max_depth': [None, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    'model__subsample': np.linspace(0.5, 1.0, 6),\n",
    "    'model__min_samples_split': [2, 3, 4],\n",
    "    'model__min_samples_leaf': [1, 2, 3]\n",
    "}\n",
    "\n",
    "base_model = GradientBoostingClassifier()\n",
    "\n",
    "model_name = \"gradient_boosting_random_search\"\n",
    "res = create_fullpipe_v2(preprocessing, base_model, model_name, param_distributions, n_iter=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_distributions = {\n",
    "    'model__solver': ['svd', 'lsqr', 'eigen'],\n",
    "    'model__shrinkage': [None, 'auto', 0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],  # effective only if solver is 'lsqr' or 'eigen'\n",
    "    'model__tol': [1e-4, 1e-3, 1e-2, 1e-1, 1]\n",
    "}\n",
    "\n",
    "base_model = LinearDiscriminantAnalysis()\n",
    "model_name = \"linear_discriminant_analysis_random_search\"\n",
    "res = create_fullpipe_v2(preprocessing, base_model, model_name, param_distributions, n_iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA SearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fullpipe_gridsearch(preprocessing, base_model, model_name, param_grid):\n",
    "    # Create a full pipeline with preprocessing and the base model\n",
    "    fullpipe = Pipeline(steps=[('preprocess', preprocessing), ('model', base_model)])\n",
    "\n",
    "    # Setup grid search\n",
    "    grid_search = GridSearchCV(\n",
    "        fullpipe, param_grid=param_grid, \n",
    "        scoring='accuracy', cv=3, verbose=3, n_jobs=-1)\n",
    "\n",
    "    # Fit model with grid search\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Best pipeline after grid search\n",
    "    best_pipeline = grid_search.best_estimator_\n",
    "\n",
    "    # Evaluate on test data\n",
    "    cv = cross_val_score(best_pipeline, X_test, y_test, cv=3, scoring='accuracy')\n",
    "    cv_mean = cv.mean()\n",
    "    cv_std = cv.std()\n",
    "\n",
    "    # save result\n",
    "    res = save_result(cv_mean, cv_std, model_name)\n",
    "\n",
    "    # pickle the best model\n",
    "    pickle.dump(best_pipeline, open(model_folder + f'{model_name}.pkl', 'wb'))\n",
    "    \n",
    "    return res, best_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA Param Grid\n",
    "params= {\n",
    "    'model__solver': ['svd', 'lsqr', 'eigen'],\n",
    "    'model__shrinkage': [None, 'auto', 0.0, 0.1, 0.2, 0.3, 0.4, 0.5],  # effective only if solver is 'lsqr' or 'eigen'\n",
    "    'model__tol': [1e-4, 1e-3, 1e-2, 1e-1, 1]\n",
    "}\n",
    "\n",
    "base_model = LinearDiscriminantAnalysis()\n",
    "model_name = \"linear_discriminant_analysis_grid_search\"\n",
    "res, best_model = create_fullpipe_gridsearch(preprocessing, base_model, model_name, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For LDA, \"tol\" is only necessary when using \"eigen\", so I will do another version going over those options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA Param Grid\n",
    "params= {\n",
    "    'model__solver': ['eigen'],\n",
    "    'model__shrinkage': [None, 'auto', 0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],  # effective only if solver is 'lsqr' or 'eigen'\n",
    "    'model__tol': [1e-4, 1e-3, 1e-2, 1e-1, 1]\n",
    "}\n",
    "\n",
    "base_model = LinearDiscriminantAnalysis()\n",
    "model_name = \"linear_discriminant_analysis_grid_search_eigen\"\n",
    "res, best_model = create_fullpipe_gridsearch(preprocessing, base_model, model_name, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients = best_model['model'].coef_[0]\n",
    "coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cats = cat_list + numerical_columns\n",
    "f_imp = coefficients\n",
    "f_imp_df = pd.DataFrame(f_imp, index = all_cats, columns = ['Importance'])\n",
    "f_imp_df = f_imp_df.sort_values('Importance', ascending = False)\n",
    "f_imp_df = f_imp_df.round(3)\n",
    "f_imp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_imp_df[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Model\n",
    "\n",
    "- Forest / Tree Models are a little more explainable, so choose one of those if they are equivalent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "res.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Trav310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f69e36f0e9b2c8d9f319b417484f14b77c91d7bef950ad448542405eb1e0e594"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
