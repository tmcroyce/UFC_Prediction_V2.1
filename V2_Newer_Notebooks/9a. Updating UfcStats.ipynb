{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Updating UFC Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib.ticker as mtick\n",
    "import sqlite3\n",
    "import seaborn as sns\n",
    "from matplotlib.pyplot import figure\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import requests     \n",
    "import shutil       \n",
    "import datetime\n",
    "from scipy.stats import norm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import requests\n",
    "import json\n",
    "from random import randint\n",
    "import  random\n",
    "import os\n",
    "from cmath import nan\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from sklearn.pipeline import Pipeline, make_pipeline, FeatureUnion\n",
    "import pickle\n",
    "from sklearn.metrics import fbeta_score\n",
    "import winsound\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_event_date_from_ufcstats(url):\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    date = soup.find('li', class_='b-list__box-list-item').text\n",
    "    date = date.replace('\\n', '')\n",
    "    ed = date.find(':')\n",
    "    date = date[ed+2:].strip()\n",
    "    return date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_details_from_ufcstats(url):\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    details = soup.find('div', class_='b-fight-details__content').text\n",
    "    details = details.replace('\\n', '')\n",
    "    deet = details.find('Details:')\n",
    "    details = details[deet+8:].strip()\n",
    "    return details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_event_title_from_ufcstats(url):\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    title = soup.find('h2', class_='b-content__title').text\n",
    "    title = title.replace('\\n', '').strip()\n",
    "    return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_event_url_from_ufcstats(url):\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    title = soup.find('h2', class_='b-content__title')\n",
    "    title = title.find('a')\n",
    "    title = title['href']\n",
    "    return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fight_urls(urls):\n",
    "\n",
    "    # get links to all fights on each event page\n",
    "\n",
    "    links = pd.DataFrame()\n",
    "    for u in urls:\n",
    "        try:\n",
    "            reqs = requests.get(u)\n",
    "            soup = BeautifulSoup(reqs.text, 'html.parser')\n",
    "            title = soup.find('h2', class_='b-content__title').text\n",
    "            title = title.replace('\\n', '')\n",
    "            date = soup.find('li', class_='b-list__box-list-item').text\n",
    "            d = date.find('Date:')\n",
    "            date = date[d+6:].replace('\\n', '')\n",
    "            for link in soup.find_all('a'):\n",
    "                # append url and event title to df\n",
    "                if 'fight-details' in link.get('href'):\n",
    "                    links = links.append({'Fight_url': link.get('href'), 'Event_title': title, 'Date' : date}, ignore_index=True)\n",
    "\n",
    "        except:\n",
    "            return 'No Fighties'\n",
    "            \n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_winner_from_ufcstats(url):\n",
    "\n",
    "    ### This function scrapes the UFCStats website for the winner of a given fight###\n",
    "\n",
    "    try:\n",
    "        page = requests.get(url)\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "        both = soup.find_all('div', class_='b-fight-details__person')\n",
    "        first = both[0].text.replace('   ', '').replace('\\n', '').strip()\n",
    "        second = both[1].text.replace('   ', '').replace('\\n', '').strip()\n",
    "        if(first.startswith('W')):\n",
    "            winner = first[3:]\n",
    "        else:\n",
    "            winner = second[3:]\n",
    "        return winner\n",
    "        \n",
    "    except:\n",
    "        return nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_event_date_from_ufcstats(url):\n",
    "    ### This function scrapes the UFCStats website for the event date of a given fight ###\n",
    "    try:\n",
    "        page = requests.get(url)\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "        date = soup.find('li', class_='b-list__box-list-item').text\n",
    "        date = date.replace('\\n', '')\n",
    "        ed = date.find(':')\n",
    "        date = date[ed+2:].strip()\n",
    "        return date\n",
    "    except:\n",
    "        return nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_event_details(event_id):\n",
    "    # This function scrapes event details from ufc stats\n",
    "\n",
    "    #try:\n",
    "        df = pd.read_html('http://www.ufcstats.com/event-details/' + event_id)\n",
    "        df = df[0]\n",
    "        # split by space and get third element\n",
    "        df['Fighter1'] = df['Fighter'].str.split('  ').str[0]\n",
    "        df['Fighter2'] = df['Fighter'].str.split('  ').str[1]\n",
    "        # fix Kd\n",
    "        df['F1_Kd'] = df['Kd'].str.split('  ').str[0]\n",
    "        df['F2_Kd'] = df['Kd'].str.split('  ').str[1]\n",
    "        # fix Str\n",
    "        df['F1_Str'] = df['Str'].str.split('  ').str[0]\n",
    "        df['F2_Str'] = df['Str'].str.split('  ').str[1]\n",
    "        # fix Td\n",
    "        df['F1_Td'] = df['Td'].str.split('  ').str[0]\n",
    "        df['F2_Td'] = df['Td'].str.split('  ').str[1]\n",
    "        # fix Sub\n",
    "        df['F1_Sub'] = df['Sub'].str.split('  ').str[0]\n",
    "        df['F2_Sub'] = df['Sub'].str.split('  ').str[1]\n",
    "\n",
    "        badcols = ['Fighter', 'Kd', 'Str', 'Td', 'Sub']\n",
    "\n",
    "        # save index+1 to new column called \"fight_num\"\n",
    "        df['fight_num'] = df.index + 1\n",
    "        df['event_id'] = event_id\n",
    "\n",
    "        # get fight links\n",
    "        page = requests.get('http://www.ufcstats.com/event-details/' + event_id)\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "        fight_linkies = soup.find_all('a')\n",
    "        fight_links = [n['href'] for n in fight_linkies]\n",
    "        fight_links = [n for n in fight_links if 'fight-details' in n]\n",
    "        # delete duplicate links\n",
    "        fight_links2 = pd.DataFrame(fight_links)\n",
    "        fight_links2 = fight_links2.drop_duplicates()\n",
    "        fight_links = fight_links2[0].tolist()\n",
    "\n",
    "        # append fight links to df\n",
    "        df['fight_link'] = fight_links\n",
    "\n",
    "        df = df.drop(badcols, axis=1)\n",
    "        df.to_csv('data/ufc_stats/events2/'+ event_id +'.csv')\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fight_totals(fight_details_url):\n",
    "    # function returns the fight totals for a given fightid\n",
    "    da_url = fight_details_url\n",
    "    fightid = da_url[da_url.find('fight-details/')+14:]\n",
    "    df = pd.read_html(da_url)\n",
    "    # Part 1: Total Fight Stats\n",
    "    totals = df[0]\n",
    "    # replace '---' with 0\n",
    "    for col in totals.columns:\n",
    "        totals[col] = totals[col].astype(str).replace({'---': 0})\n",
    "        totals[col] = totals[col].astype(str).replace({'--': 0})\n",
    "\n",
    "\n",
    "    totals['Fighter_A'] = totals['Fighter'].str.split(' ').str[0] + ' ' + totals['Fighter'].str.split(' ').str[1]\n",
    "    totals['Fighter_B'] = totals['Fighter'].str.split(' ').str[2] + ' ' + totals['Fighter'].str.split(' ').str[3]\n",
    "    totals['A_Kd'] = totals['KD'].str.split('  ').str[0]\n",
    "    totals['B_Kd'] = totals['KD'].str.split('  ').str[1]\n",
    "    totals['A_Sig_strike'] = totals['Sig. str.'].str.split('  ').str[0]\n",
    "    totals['B_Sig_strike'] = totals['Sig. str.'].str.split('  ').str[1]\n",
    "    # split sig strikes into landed and attempted\n",
    "    totals['A_Sig_strike_land'] = totals['A_Sig_strike'].str.split(' of ').str[0]\n",
    "    totals['A_Sig_strike_att'] = totals['A_Sig_strike'].str.split(' of ').str[1]\n",
    "    totals['B_Sig_strike_land'] = totals['B_Sig_strike'].str.split(' of ').str[0]\n",
    "    totals['B_Sig_strike_att'] = totals['B_Sig_strike'].str.split(' of ').str[1]\n",
    "    # change to numeric\n",
    "    totals['A_Sig_strike_land'].replace({'---', 0}, inplace=True)\n",
    "    totals['A_Sig_strike_att'].replace({'---', 0}, inplace=True)\n",
    "    totals['B_Sig_strike_land'].replace({'---', 0}, inplace=True)\n",
    "    totals['B_Sig_strike_att'].replace({'---', 0}, inplace=True)\n",
    "\n",
    "    totals['A_Sig_strike_land'] = pd.to_numeric(totals['A_Sig_strike_land'])\n",
    "    totals['A_Sig_strike_att'] = pd.to_numeric(totals['A_Sig_strike_att'])\n",
    "    totals['B_Sig_strike_land'] = pd.to_numeric(totals['B_Sig_strike_land'])\n",
    "    totals['B_Sig_strike_att'] = pd.to_numeric(totals['B_Sig_strike_att'])\n",
    "\n",
    "    totals['A_Sig_strike_percent'] = totals['Sig. str. %'].str.split('  ').str[0]\n",
    "    totals['B_Sig_strike_percent'] = totals['Sig. str. %'].str.split('  ').str[1]\n",
    "    # get rid of % sign\n",
    "    totals['A_Sig_strike_percent'] = totals['A_Sig_strike_percent'].str.replace('%', '')\n",
    "    totals['B_Sig_strike_percent'] = totals['B_Sig_strike_percent'].str.replace('%', '')\n",
    "\n",
    "    totals['A_Sig_strike_percent'] = totals['A_Sig_strike_percent'].astype(str).replace({'---': 0})\n",
    "    totals['B_Sig_strike_percent'] = totals['B_Sig_strike_percent'].astype(str).replace({'---': 0})\n",
    "    # change to numeric\n",
    "    totals['A_Sig_strike_percent'] = pd.to_numeric(totals['A_Sig_strike_percent'])/100\n",
    "    totals['B_Sig_strike_percent'] = pd.to_numeric(totals['B_Sig_strike_percent'])/100\n",
    "    # total strikes\n",
    "    totals['A_Total_Strikes'] = totals['Total str.'].str.split('  ').str[0]\n",
    "    totals['B_Total_Strikes'] = totals['Total str.'].str.split('  ').str[1]\n",
    "    # split total strikes into landed and attempted\n",
    "    totals['A_Total_Strikes_land'] = totals['A_Total_Strikes'].str.split(' of ').str[0]\n",
    "    totals['A_Total_Strikes_att'] = totals['A_Total_Strikes'].str.split(' of ').str[1]\n",
    "    totals['B_Total_Strikes_land'] = totals['B_Total_Strikes'].str.split(' of ').str[0]\n",
    "    totals['B_Total_Strikes_att'] = totals['B_Total_Strikes'].str.split(' of ').str[1]\n",
    "\n",
    "    totals['A_Total_Strikes_land'].replace({'---',0}, inplace=True)\n",
    "    totals['A_Total_Strikes_att'].replace({'---',0}, inplace=True)\n",
    "    totals['B_Total_Strikes_land'].replace({'---',0}, inplace=True)\n",
    "    totals['B_Total_Strikes_att'].replace({'---',0}, inplace=True)\n",
    "    # change to numeric\n",
    "    totals['A_Total_Strikes_land'] = pd.to_numeric(totals['A_Total_Strikes_land'])\n",
    "    totals['A_Total_Strikes_att'] = pd.to_numeric(totals['A_Total_Strikes_att'])\n",
    "    totals['B_Total_Strikes_land'] = pd.to_numeric(totals['B_Total_Strikes_land'])\n",
    "    totals['B_Total_Strikes_att'] = pd.to_numeric(totals['B_Total_Strikes_att'])\n",
    "\n",
    "    totals['A_Total_Strikes_land'].astype(str).replace({'---',0}, inplace=True)\n",
    "    totals['A_Total_Strikes_att'].astype(str).replace({'---',0}, inplace=True)\n",
    "    totals['B_Total_Strikes_land'].astype(str).replace({'---',0}, inplace=True)\n",
    "    totals['B_Total_Strikes_att'].astype(str).replace({'---',0}, inplace=True) \n",
    "\n",
    "    # calculate total strike percentage\n",
    "    totals['A_Total_Strikes_percent'] = totals['A_Total_Strikes_land'] / totals['A_Total_Strikes_att']\n",
    "    totals['B_Total_Strikes_percent'] = totals['B_Total_Strikes_land'] / totals['B_Total_Strikes_att']\n",
    "    # takedown stats\n",
    "    totals['A_Takedowns'] = totals['Td'].str.split('  ').str[0]\n",
    "    totals['B_Takedowns'] = totals['Td'].str.split('  ').str[1]\n",
    "    # split takedowns into landed and attempted\n",
    "    totals['A_Takedowns_land'] = totals['A_Takedowns'].str.split(' of ').str[0]\n",
    "    totals['A_Takedowns_att'] = totals['A_Takedowns'].str.split(' of ').str[1]\n",
    "    totals['B_Takedowns_land'] = totals['B_Takedowns'].str.split(' of ').str[0]\n",
    "    totals['B_Takedowns_att'] = totals['B_Takedowns'].str.split(' of ').str[1]\n",
    "    # change to numeric\n",
    "    totals['A_Takedowns_land'] = pd.to_numeric(totals['A_Takedowns_land'])\n",
    "    totals['A_Takedowns_att'] = pd.to_numeric(totals['A_Takedowns_att'])\n",
    "    totals['B_Takedowns_land'] = pd.to_numeric(totals['B_Takedowns_land'])\n",
    "    totals['B_Takedowns_att'] = pd.to_numeric(totals['B_Takedowns_att'])\n",
    "    # fix %\n",
    "    totals['Td %'].replace({'---': 0}, inplace=True)\n",
    "    # get rid of ---\n",
    "    totals['A_Takedowns_land'].replace({'---': 0}, inplace=True)\n",
    "    totals['A_Takedowns_att'].replace({'---': 0}, inplace=True)\n",
    "    totals['B_Takedowns_land'].replace({'---': 0}, inplace=True)\n",
    "    totals['B_Takedowns_att'].replace({'---': 0}, inplace=True)\n",
    "    totals['A_Takedown_percent'] = totals['A_Takedowns_land'] / totals['A_Takedowns_att']\n",
    "    totals['B_Takedown_percent'] = totals['B_Takedowns_land'] / totals['B_Takedowns_att']\n",
    "    # submission attempts\n",
    "    totals['A_Sub_Attempts'] = totals['Sub. att'].str.split('  ').str[0]\n",
    "    totals['B_Sub_Attempts'] = totals['Sub. att'].str.split('  ').str[1]\n",
    "    # split submission attempts into landed and attempted\n",
    "    totals['A_Sub_Attempts_land'] = totals['A_Sub_Attempts'].str.split(' of ').str[0]\n",
    "    totals['A_Sub_Attempts_att'] = totals['A_Sub_Attempts'].str.split(' of ').str[1]\n",
    "    totals['B_Sub_Attempts_land'] = totals['B_Sub_Attempts'].str.split(' of ').str[0]\n",
    "    totals['B_Sub_Attempts_att'] = totals['B_Sub_Attempts'].str.split(' of ').str[1]\n",
    "    # change to numeric\n",
    "    totals['A_Sub_Attempts_land'] = pd.to_numeric(totals['A_Sub_Attempts_land'])\n",
    "    totals['A_Sub_Attempts_att'] = pd.to_numeric(totals['A_Sub_Attempts_att'])\n",
    "    totals['B_Sub_Attempts_land'] = pd.to_numeric(totals['B_Sub_Attempts_land'])\n",
    "    totals['B_Sub_Attempts_att'] = pd.to_numeric(totals['B_Sub_Attempts_att'])\n",
    "    # Rev\n",
    "    totals['A_Rev'] = totals['Rev.'].str.split('  ').str[0]\n",
    "    totals['B_Rev'] = totals['Rev.'].str.split('  ').str[1]\n",
    "    # change to numeric\n",
    "    totals['A_Rev'] = pd.to_numeric(totals['A_Rev'])\n",
    "    totals['B_Rev'] = pd.to_numeric(totals['B_Rev'])\n",
    "    # control time\n",
    "    totals['A_Ctrl_time'] = totals['Ctrl'].str.split('  ').str[0]\n",
    "    totals['B_Ctrl_time'] = totals['Ctrl'].str.split('  ').str[1]\n",
    "    \n",
    "\n",
    "    # split control time into minutes and seconds, and then convert to seconds\n",
    "    totals['A_Ctrl_time_min'] = totals['A_Ctrl_time'].str.split(':').str[0]\n",
    "    totals['A_Ctrl_time_sec'] = totals['A_Ctrl_time'].str.split(':').str[1]\n",
    "    totals['B_Ctrl_time_min'] = totals['B_Ctrl_time'].str.split(':').str[0]\n",
    "    totals['B_Ctrl_time_sec'] = totals['B_Ctrl_time'].str.split(':').str[1]\n",
    "\n",
    "    totals['A_Ctrl_time_min'] = totals['A_Ctrl_time_min'].replace({'--': 0})\n",
    "    totals['A_Ctrl_time_sec'] = totals['A_Ctrl_time_sec'].replace({'--': 0})\n",
    "    totals['B_Ctrl_time_min'] = totals['B_Ctrl_time_min'].replace({'--': 0})\n",
    "    totals['B_Ctrl_time_sec'] = totals['B_Ctrl_time_sec'].replace({'--': 0})\n",
    "\n",
    "    # convert to seconds\n",
    "    totals['A_Ctrl_time_tot'] = pd.to_numeric(totals['A_Ctrl_time_min']) * 60 + pd.to_numeric(totals['A_Ctrl_time_sec'])\n",
    "    totals['B_Ctrl_time_tot'] = pd.to_numeric(totals['B_Ctrl_time_min']) * 60 + pd.to_numeric(totals['B_Ctrl_time_sec'])\n",
    "\n",
    "\n",
    "    # add date with beatufil soup\n",
    "    totals['details'] = get_details_from_ufcstats(da_url)\n",
    "    totals['event_title'] = get_event_title_from_ufcstats(da_url)\n",
    "    event_url = get_event_url_from_ufcstats(da_url)\n",
    "    totals['event_url'] = event_url\n",
    "    totals['date'] = get_event_date_from_ufcstats(event_url)\n",
    "\n",
    "    try:\n",
    "        totals['Winner']= get_winner_from_ufcstats(da_url)\n",
    "    except:\n",
    "        totals['Winner']= nan\n",
    "\n",
    "    bad_cols = ['Fighter', 'KD', 'Sig. str. %', 'Sig. str.', 'Total str.', 'Td', 'Td %', 'Sub. att', \n",
    "                'Rev.', 'Ctrl', 'A_Sig_strike', 'B_Sig_strike', 'A_Takedowns', 'B_Takedowns',\n",
    "                'A_Sub_Attempts', 'B_Sub_Attempts', 'A_Ctrl_time', 'B_Ctrl_time']\n",
    "    totals.drop(columns=bad_cols, inplace=True)\n",
    "    totals.to_csv('data/ufc_stats/fight_totals2/' + fightid + '_totals.csv')\n",
    "    return totals\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_significant_strikes(url):\n",
    "    df = pd.read_html(url)\n",
    "    # Part 2: Significant Strikes\n",
    "    ss = df[2]\n",
    "     \n",
    "    ss['Fighter_A'] = ss['Fighter'].str.split(' ').str[0] + ' ' + ss['Fighter'].str.split(' ').str[1]\n",
    "    ss['Fighter_B'] = ss['Fighter'].str.split(' ').str[2] + ' ' + ss['Fighter'].str.split(' ').str[3]\n",
    "    ss['A_Head_Strikes'] = ss['Head'].str.split('  ').str[0]\n",
    "    ss['B_Head_Strikes'] = ss['Head'].str.split('  ').str[1]\n",
    "    # split head strikes into landed and attempted\n",
    "    ss['A_Head_Strikes_land'] = ss['A_Head_Strikes'].str.split(' of ').str[0]\n",
    "    ss['A_Head_Strikes_att'] = ss['A_Head_Strikes'].str.split(' of ').str[1]\n",
    "    ss['B_Head_Strikes_land'] = ss['B_Head_Strikes'].str.split(' of ').str[0]\n",
    "    ss['B_Head_Strikes_att'] = ss['B_Head_Strikes'].str.split(' of ').str[1]\n",
    "    # change to numeric\n",
    "    ss['A_Head_Strikes_land'] = pd.to_numeric(ss['A_Head_Strikes_land'])\n",
    "    ss['A_Head_Strikes_att'] = pd.to_numeric(ss['A_Head_Strikes_att'])\n",
    "    ss['B_Head_Strikes_land'] = pd.to_numeric(ss['B_Head_Strikes_land'])\n",
    "    ss['B_Head_Strikes_att'] = pd.to_numeric(ss['B_Head_Strikes_att'])\n",
    "    # Head Strikes Percentage\n",
    "    ss['A_Head_Strikes_percent'] = ss['A_Head_Strikes_land'] / ss['A_Head_Strikes_att']\n",
    "    ss['B_Head_Strikes_percent'] = ss['B_Head_Strikes_land'] / ss['B_Head_Strikes_att']\n",
    "    # Body Strikes\n",
    "    ss['A_Body_Strikes'] = ss['Body'].str.split('  ').str[0]\n",
    "    ss['B_Body_Strikes'] = ss['Body'].str.split('  ').str[1]\n",
    "    # split body strikes into landed and attempted\n",
    "    ss['A_Body_Strikes_land'] = ss['A_Body_Strikes'].str.split(' of ').str[0]\n",
    "    ss['A_Body_Strikes_att'] = ss['A_Body_Strikes'].str.split(' of ').str[1]\n",
    "    ss['B_Body_Strikes_land'] = ss['B_Body_Strikes'].str.split(' of ').str[0]\n",
    "    ss['B_Body_Strikes_att'] = ss['B_Body_Strikes'].str.split(' of ').str[1]\n",
    "    # change to numeric\n",
    "    ss['A_Body_Strikes_land'] = pd.to_numeric(ss['A_Body_Strikes_land'])\n",
    "    ss['A_Body_Strikes_att'] = pd.to_numeric(ss['A_Body_Strikes_att'])\n",
    "    ss['B_Body_Strikes_land'] = pd.to_numeric(ss['B_Body_Strikes_land'])\n",
    "    ss['B_Body_Strikes_att'] = pd.to_numeric(ss['B_Body_Strikes_att'])\n",
    "    # Body Strikes Percentage\n",
    "    ss['A_Body_Strikes_percent'] = ss['A_Body_Strikes_land'] / ss['A_Body_Strikes_att']\n",
    "    ss['B_Body_Strikes_percent'] = ss['B_Body_Strikes_land'] / ss['B_Body_Strikes_att']\n",
    "    # Leg Strikes\n",
    "    ss['A_Leg_Strikes'] = ss['Leg'].str.split('  ').str[0]\n",
    "    ss['B_Leg_Strikes'] = ss['Leg'].str.split('  ').str[1]\n",
    "    # split leg strikes into landed and attempted\n",
    "    ss['A_Leg_Strikes_land'] = ss['A_Leg_Strikes'].str.split(' of ').str[0]\n",
    "    ss['A_Leg_Strikes_att'] = ss['A_Leg_Strikes'].str.split(' of ').str[1]\n",
    "    ss['B_Leg_Strikes_land'] = ss['B_Leg_Strikes'].str.split(' of ').str[0]\n",
    "    ss['B_Leg_Strikes_att'] = ss['B_Leg_Strikes'].str.split(' of ').str[1]\n",
    "    # change to numeric\n",
    "    ss['A_Leg_Strikes_land'] = pd.to_numeric(ss['A_Leg_Strikes_land'])\n",
    "    ss['A_Leg_Strikes_att'] = pd.to_numeric(ss['A_Leg_Strikes_att'])\n",
    "    ss['B_Leg_Strikes_land'] = pd.to_numeric(ss['B_Leg_Strikes_land'])\n",
    "    ss['B_Leg_Strikes_att'] = pd.to_numeric(ss['B_Leg_Strikes_att'])\n",
    "    # Leg Strikes Percentage\n",
    "    ss['A_Leg_Strikes_percent'] = ss['A_Leg_Strikes_land'] / ss['A_Leg_Strikes_att']\n",
    "    ss['B_Leg_Strikes_percent'] = ss['B_Leg_Strikes_land'] / ss['B_Leg_Strikes_att']\n",
    "    # Distance Strikes\n",
    "    ss['A_Distance_Strikes'] = ss['Distance'].str.split('  ').str[0]\n",
    "    ss['B_Distance_Strikes'] = ss['Distance'].str.split('  ').str[1]\n",
    "    # split distance strikes into landed and attempted\n",
    "    ss['A_Distance_Strikes_land'] = ss['A_Distance_Strikes'].str.split(' of ').str[0]\n",
    "    ss['A_Distance_Strikes_att'] = ss['A_Distance_Strikes'].str.split(' of ').str[1]\n",
    "    ss['B_Distance_Strikes_land'] = ss['B_Distance_Strikes'].str.split(' of ').str[0]\n",
    "    ss['B_Distance_Strikes_att'] = ss['B_Distance_Strikes'].str.split(' of ').str[1]\n",
    "    # change to numeric\n",
    "    ss['A_Distance_Strikes_land'] = pd.to_numeric(ss['A_Distance_Strikes_land'])\n",
    "    ss['A_Distance_Strikes_att'] = pd.to_numeric(ss['A_Distance_Strikes_att'])\n",
    "    ss['B_Distance_Strikes_land'] = pd.to_numeric(ss['B_Distance_Strikes_land'])\n",
    "    ss['B_Distance_Strikes_att'] = pd.to_numeric(ss['B_Distance_Strikes_att'])\n",
    "    # distance strikes percentage\n",
    "    ss['A_Distance_Strikes_percent'] = ss['A_Distance_Strikes_land'] / ss['A_Distance_Strikes_att']\n",
    "    ss['B_Distance_Strikes_percent'] = ss['B_Distance_Strikes_land'] / ss['B_Distance_Strikes_att']\n",
    "\n",
    "    # Clinch Strikes\n",
    "    ss['A_Clintch_Strikes'] = ss['Clinch'].str.split('  ').str[0]\n",
    "    ss['B_Clintch_Strikes'] = ss['Clinch'].str.split('  ').str[1]\n",
    "    # split clinch strikes into landed and attempted\n",
    "    ss['A_Clinch_Strikes_land'] = ss['A_Clintch_Strikes'].str.split(' of ').str[0]\n",
    "    ss['A_Clinch_Strikes_att'] = ss['A_Clintch_Strikes'].str.split(' of ').str[1]\n",
    "    ss['B_Clinch_Strikes_land'] = ss['B_Clintch_Strikes'].str.split(' of ').str[0]\n",
    "    ss['B_Clinch_Strikes_att'] = ss['B_Clintch_Strikes'].str.split(' of ').str[1]\n",
    "    # change to numeric\n",
    "    ss['A_Clinch_Strikes_land'] = pd.to_numeric(ss['A_Clinch_Strikes_land'])\n",
    "    ss['A_Clinch_Strikes_att'] = pd.to_numeric(ss['A_Clinch_Strikes_att'])\n",
    "    ss['B_Clinch_Strikes_land'] = pd.to_numeric(ss['B_Clinch_Strikes_land'])\n",
    "    ss['B_Clinch_Strikes_att'] = pd.to_numeric(ss['B_Clinch_Strikes_att'])\n",
    "    # clinch strikes percentage\n",
    "    ss['A_Clinch_Strikes_percent'] = ss['A_Clinch_Strikes_land'] / ss['A_Clinch_Strikes_att']\n",
    "    ss['B_Clinch_Strikes_percent'] = ss['B_Clinch_Strikes_land'] / ss['B_Clinch_Strikes_att']\n",
    "\n",
    "    # Ground Strikes\n",
    "    ss['A_Ground_Strikes'] = ss['Ground'].str.split('  ').str[0]\n",
    "    ss['B_Ground_Strikes'] = ss['Ground'].str.split('  ').str[1]\n",
    "    # split ground strikes into landed and attempted\n",
    "    ss['A_Ground_Strikes_land'] = ss['A_Ground_Strikes'].str.split(' of ').str[0]\n",
    "    ss['A_Ground_Strikes_att'] = ss['A_Ground_Strikes'].str.split(' of ').str[1]\n",
    "    ss['B_Ground_Strikes_land'] = ss['B_Ground_Strikes'].str.split(' of ').str[0]\n",
    "    ss['B_Ground_Strikes_att'] = ss['B_Ground_Strikes'].str.split(' of ').str[1]\n",
    "    # change to numeric\n",
    "    ss['A_Ground_Strikes_land'] = pd.to_numeric(ss['A_Ground_Strikes_land'])\n",
    "    ss['A_Ground_Strikes_att'] = pd.to_numeric(ss['A_Ground_Strikes_att'])\n",
    "    ss['B_Ground_Strikes_land'] = pd.to_numeric(ss['B_Ground_Strikes_land'])\n",
    "    ss['B_Ground_Strikes_att'] = pd.to_numeric(ss['B_Ground_Strikes_att'])\n",
    "    # ground strikes percentage\n",
    "    ss['A_Ground_Strikes_percent'] = ss['A_Ground_Strikes_land'] / ss['A_Ground_Strikes_att']\n",
    "    ss['B_Ground_Strikes_percent'] = ss['B_Ground_Strikes_land'] / ss['B_Ground_Strikes_att']\n",
    "    # add stuff with beautiful soup\n",
    "    ss['details'] = get_details_from_ufcstats(url)\n",
    "    ss['event_title'] = get_event_title_from_ufcstats(url)\n",
    "    event_url = get_event_url_from_ufcstats(url)\n",
    "    ss['event_url'] = event_url\n",
    "\n",
    "    fightid = url[url.find('fight-details/')+14:]\n",
    "\n",
    "    bad_cols = ['Fighter', 'Sig. str', 'Sig. str. %', 'Head', 'Body', 'Leg', 'Distance', 'Clinch', 'Ground',\n",
    "                'A_Head_Strikes', 'B_Head_Strikes', 'A_Body_Strikes', 'B_Body_Strikes', 'A_Leg_Strikes', 'B_Leg_Strikes',\n",
    "                'A_Distance_Strikes', 'B_Distance_Strikes', 'A_Clintch_Strikes', 'B_Clintch_Strikes', 'A_Ground_Strikes', \n",
    "                'B_Ground_Strikes']\n",
    "    ss.drop(columns=bad_cols, inplace=True)\n",
    "    ss.to_csv('data/ufc_stats/sig_strikes2/' + fightid + '_sigstrikes.csv')\n",
    "\n",
    "    return ss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update Event Data (check against files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all_event_data\n",
    "all_event_data = pd.read_csv('data/final/events/All_Events_Fights_and_FightUrls.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inital_events_chosen = all_event_data['event_id'].unique()\n",
    "print(len(inital_events_chosen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inital_events_chosen = inital_events_chosen.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_recent_events():\n",
    "    ufc_stats_events = \"http://ufcstats.com/statistics/events/completed?page=all\"\n",
    "\n",
    "    # for each event, get the event link\n",
    "    page = requests.get(ufc_stats_events)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    event_linkies = soup.find_all('a', class_='b-link b-link_style_black')\n",
    "\n",
    "    # list of event links and event titles\n",
    "    event_links = [n['href'] for n in event_linkies]\n",
    "    event_titles = [n.text for n in event_linkies]\n",
    "    # replace '\\n' with ''\n",
    "    event_titles = [n.replace('\\n', '').strip() for n in event_titles]\n",
    "\n",
    "    # make df\n",
    "    event_df = pd.DataFrame({'event_title': event_titles, 'event_url': event_links})\n",
    "    # take most recent 5\n",
    "    ed = event_df.head(5)\n",
    "\n",
    "    # return 5 most recent\n",
    "    return ed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_recent_events = get_most_recent_events()\n",
    "most_recent_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add event_id\n",
    "most_recent_events['event_id'] = most_recent_events['event_url'].str.split('/').str[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completed_ufcstats_event_ids = most_recent_events['event_id'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_missing_events(completed_ufcstats_event_ids):\n",
    "    \n",
    "    # check the list of completed events against the events2 folder\n",
    "    # make list of events that are no downloading\n",
    "\n",
    "    missing = []\n",
    "    for event in completed_ufcstats_event_ids:\n",
    "        if event + '.csv' not in os.listdir('data/ufc_stats/events2'):\n",
    "            missing.append(event)\n",
    "\n",
    "    return missing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_events = check_for_missing_events(completed_ufcstats_event_ids)\n",
    "missing_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just this one time\n",
    "missing_events = missing_events[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update by downloading events\n",
    "for event in missing_events:\n",
    "    get_event_details(event)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For each missing event, download all fights data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ufcstats_fight_urls(missing_event_df):\n",
    "\n",
    "    # For each missing event, download the fight url links\n",
    "\n",
    "    links = pd.DataFrame()\n",
    "    for event in missing_event_df:\n",
    "        data = pd.read_csv('data/ufc_stats/events2/'+ event +'.csv')\n",
    "        links = links.append(data[['Fighter1', 'Fighter2','fight_link', 'event_id']], ignore_index=True)\n",
    "    return links\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_fights = get_ufcstats_fight_urls(missing_events)\n",
    "missing_fights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(missing_fights) > 0:\n",
    "    missing_fight_links = missing_fights['fight_link'].tolist()\n",
    "    for fight_link in missing_fight_links:\n",
    "        \n",
    "        # get totals\n",
    "        get_fight_totals(fight_link)\n",
    "        get_significant_strikes(fight_link)\n",
    "\n",
    "else:\n",
    "    print('No missing fights')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update for NEXT Fight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to return the next 3 UFC events using BeautifulSoup\n",
    "def get_next_events(url):\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    # get events\n",
    "    event1 = soup.find('div', class_='c-card-event--result__info')\n",
    "    event1_txt = soup.find('div', class_='c-card-event--result__info').text\n",
    "    event1_url = event1.find('a')['href']\n",
    "    event1_url = 'https://www.ufc.com' + event1_url\n",
    "    event1_title = event1_txt.split('\\n')[1]\n",
    "    event1_time = event1_txt.split('/')[1]\n",
    "\n",
    "    data = pd.DataFrame({'event_title': [event1_title], 'event_url': [event1_url], 'event_date': [event1_time]})\n",
    "\n",
    "    event2 = soup.find('div', class_='c-card-event--result__info').find_next('div', class_='c-card-event--result__info')\n",
    "    event2_txt = soup.find('div', class_='c-card-event--result__info').find_next('div', class_='c-card-event--result__info').text\n",
    "    event2_url = event2.find('a')['href']\n",
    "    event2_url = 'https://www.ufc.com' + event2_url\n",
    "    event2_title = event2_txt.split('\\n')[1]\n",
    "    event2_time = event2_txt.split('/')[1]\n",
    "\n",
    "\n",
    "    data = data.append({'event_title': event2_title, 'event_url': event2_url, 'event_date': event2_time}, ignore_index=True)\n",
    "    \n",
    "    event3 = soup.find('div', class_='c-card-event--result__info').find_next('div', class_='c-card-event--result__info').find_next('div', class_='c-card-event--result__info')\n",
    "    event3_txt = soup.find('div', class_='c-card-event--result__info').find_next('div', class_='c-card-event--result__info').find_next('div', class_='c-card-event--result__info').text\n",
    "    event3_url = event3.find('a')['href']\n",
    "    event3_url = 'https://www.ufc.com' + event3_url\n",
    "    event3_title = event3_txt.split('\\n')[1]\n",
    "    event3_time = event3_txt.split('/')[1]\n",
    "\n",
    "    data = data.append({'event_title': event3_title, 'event_url': event3_url, 'event_date': event3_time}, ignore_index=True)\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "# Function to get the fight card for a given event using BeautifulSoup\n",
    "def get_event_fights(event_url):\n",
    "    page = requests.get(event_url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    # get main card, fight 1\n",
    "\n",
    "    mcn = soup.find_all('li', class_='l-listing__item')\n",
    "    # get num of mc\n",
    "    num_mc = len(mcn)\n",
    "    # for each mc, do the following\n",
    "    data = pd.DataFrame()\n",
    "    n = 0\n",
    "    for i in mcn:\n",
    "        mc = mcn[n]\n",
    "        # fight 1\n",
    "        fighter1= mc.find('div', class_ ='c-listing-fight__corner-name c-listing-fight__corner-name--red').text\n",
    "        fighter1 = fighter1.replace('\\n', ' ')\n",
    "        fighter1 = fighter1.strip()\n",
    "        fighter2 = mc.find('div', class_ ='c-listing-fight__corner-name c-listing-fight__corner-name--blue').text\n",
    "        fighter2 = fighter2.replace('\\n', ' ')\n",
    "        fighter2 = fighter2.strip()\n",
    "        weightclass = mc.find('div', class_='c-listing-fight__class-text').text\n",
    "        fighter1_odds = mc.find('span', class_='c-listing-fight__odds').text\n",
    "        fighter2_odds = mc.find('span', class_='c-listing-fight__odds').find_next('span', class_='c-listing-fight__odds').text\n",
    "        fighter1_odds = fighter1_odds.replace('\\n', '')\n",
    "        fighter2_odds = fighter2_odds.replace('\\n', '')\n",
    "        # fighter odds to float\n",
    "        if (fighter1_odds == '-') :\n",
    "            fighter1_odds = nan\n",
    "        if (fighter2_odds == '-') :\n",
    "            fighter2_odds = nan\n",
    "\n",
    "        data = data.append({'fighter1': fighter1, 'fighter2': fighter2, 'weightclass': weightclass, \n",
    "                            'fighter1_odds': fighter1_odds, 'fighter2_odds': fighter2_odds}, ignore_index=True)\n",
    "        n = n + 1\n",
    "    return data\n",
    "\n",
    "\n",
    "# get next events if event fighter data is not na\n",
    "def get_next_events2(url):\n",
    "    data = get_next_events(url)\n",
    "    for i in range(0, len(data)):\n",
    "        event_url = data['event_url'][i]\n",
    "        event_fights = get_event_fights(event_url)\n",
    "        if (len(event_fights) == 0):\n",
    "            data = data.drop(i)\n",
    "    return data\n",
    "\n",
    "# get next events from UFCStats.com using BS\n",
    "def get_next_event_ufcstats():\n",
    "    url = 'http://www.ufcstats.com/statistics/events/upcoming'\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    # get events\n",
    "    event1 = soup.find('td', class_='b-statistics__table-col')\n",
    "    event1_txt = soup.find('td', class_='b-statistics__table-col').text\n",
    "    event_txt = event1_txt.replace('   ', '').replace('\\n', '').strip()\n",
    "    event_title = event_txt.split('  ')[0]\n",
    "    event_date = event_txt.split('  ')[1]\n",
    "    event1_url = event1.find('a')['href']\n",
    "    data = pd.DataFrame({'event_title': [event_title], 'event_url': [event1_url], 'event_date': [event_date]})\n",
    "    return data\n",
    "\n",
    "\n",
    "# get fighter urls from UFCStats.com using BS\n",
    "def get_fighter_urls(event_details_url):\n",
    "    page = requests.get(event_details_url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    # get events\n",
    "    events = soup.find_all('tr', class_='b-fight-details__table-row b-fight-details__table-row__hover js-fight-details-click')\n",
    "    n = 0\n",
    "    next_event_data = pd.DataFrame()\n",
    "\n",
    "    for event in events:\n",
    "        fighters = events[n].find_all('p', class_='b-fight-details__table-text')\n",
    "        fighter1 = fighters[0].text\n",
    "        fighter1 = fighter1.replace('  ', '').replace('\\n', '').strip()\n",
    "        fighter2 = fighters[1].text\n",
    "        fighter2 = fighter2.replace('  ', '').replace('\\n', '').strip()\n",
    "        fighter1_url = fighters[0].find('a')['href']\n",
    "        fighter2_url = fighters[1].find('a')['href']\n",
    "        next_event_data = next_event_data.append({'fighter1' :fighter1, 'fighter2:' : fighter2, 'fighter1_url': fighter1_url, 'fighter2_url':fighter2_url, 'fight#' : n+1}, ignore_index = True)\n",
    "        n += 1\n",
    "\n",
    "    return next_event_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_ufc_events = get_next_events('https://www.ufc.com/events')\n",
    "next_ufc_events"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get event Fights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get event fights for first event\n",
    "event_url = next_ufc_events['event_url'][0]\n",
    "event_fights = get_event_fights(event_url)\n",
    "event_fights"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get secret number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find secret number in ufc events using BS & Selenium\n",
    "def get_secret_number(event_url):\n",
    "    # if no driver open, open one\n",
    "    driver = None\n",
    "    if (driver == None):\n",
    "        driver = webdriver.Chrome('C:\\\\Users\\\\Travis\\\\OneDrive\\\\Data Science\\\\Personal_Projects\\\\Sports\\\\UFC_Prediction_V2\\\\chromedriver.exe')\n",
    "    \n",
    "    driver.get(event_url)\n",
    "    # sleep for 15 seconds\n",
    "    time.sleep(5)\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    pretty = soup.prettify()\n",
    "\n",
    "    # find first data-fmid to get first matchup\n",
    "    fmid_start = pretty.find('data-fmid')\n",
    "    fmid = pretty[fmid_start+11:fmid_start+16]\n",
    "\n",
    "    # go to event url with fmid\n",
    "    driver.get(event_url +'#' + fmid)\n",
    "    time.sleep(8)\n",
    "\n",
    "    # find all links within page\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    # find all iframe src\n",
    "    iframe = soup.find_all('iframe')\n",
    "\n",
    "    # find all links\n",
    "    iframe_text = str(iframe)\n",
    "    matchup = iframe_text.find('matchup')\n",
    "    # matchup_url = iframe_text[matchup+8:matchup+12]\n",
    "    # print('matchup_url: ' + matchup_url)\n",
    "    # secret_number = matchup_url\n",
    "    return matchup, fmid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "secret_number_1, fmid_1 = get_secret_number(event_url)\n",
    "print('secret_number_1: ' + str(secret_number_1))\n",
    "print('fmid_1: ' + str(fmid_1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.ufc.com/matchup/' + str(secret_number_1) + '/' + str(fmid_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def just_get_soup(matchup_url):\n",
    "    response = requests.get(matchup_url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    return soup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get matchup data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab_soup(match_url):\n",
    "    # if no driver open, open one\n",
    "    driver = None\n",
    "    if (driver == None):\n",
    "        driver = webdriver.Chrome('C:\\\\Users\\\\Travis\\\\OneDrive\\\\Data Science\\\\Personal_Projects\\\\Sports\\\\UFC_Prediction_V2\\\\chromedriver.exe')\n",
    "    \n",
    "    driver.get(match_url)\n",
    "    # sleep for 5 seconds\n",
    "    time.sleep(5)\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    # find class=\"c-listing-fight__details-content details-content\"\n",
    "\n",
    "    #chosen_fight = soup.find('div', class_='c-listing-fight__details-content details-content')\n",
    "    \n",
    "    # get inner html\n",
    "    inner_html = driver.execute_script(\"return document.body.innerHTML\")\n",
    "    chosen_fight = inner_html.find('c-listing-fight__details-content details-content')\n",
    "    chosen_fight = inner_html[chosen_fight:chosen_fight+1000]\n",
    "\n",
    "\n",
    "    # prettify inner html\n",
    "    soup2 = BeautifulSoup(inner_html, 'html.parser')\n",
    "    pretty = soup2.prettify()\n",
    "\n",
    "\n",
    "    return chosen_fight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to scrape UFC fight data\n",
    "def grab_matchup_data(matchup_url):\n",
    "    response = requests.get(matchup_url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser').text\n",
    "    soup = soup.replace('   ', '').replace('\\n', '')\n",
    "\n",
    "    od = soup.find('Odds')\n",
    "    rec = soup.find('Record')\n",
    "    a_record = soup[od + 5 : rec - 2]\n",
    "    last_fight = soup.find(\"Last Fight\")\n",
    "    b_record = soup[rec + 7 : last_fight - 5]\n",
    "\n",
    "    hite = soup.find('Height')\n",
    "    f = soup.find(\"' \")\n",
    "    a_height = soup[f -1 : hite - 2]\n",
    "    # find second occurance of f\n",
    "    f2 = soup.find(\"' \", f + 1)\n",
    "    b_height = soup[hite + 7 : f2+5]\n",
    "\n",
    "    # Find reach\n",
    "    reach = soup.find('Reach')\n",
    "    # find second occurance of \"LB\"\n",
    "    lb = soup.find('LB')\n",
    "    lb2 = soup.find('LB', lb + 1)\n",
    "    a_reach = soup[lb2 +5 : reach ]\n",
    "    inn = soup.find(\"in \")\n",
    "    # get the word after reach\n",
    "    big_space = soup.find('  ', reach + 1)\n",
    "    b_reach = soup[reach + 6 : big_space]\n",
    "\n",
    "    # Find Leg Reach\n",
    "    leg = soup.find('Leg Reach')\n",
    "    big_space2 = soup.find('  ', big_space + 1)\n",
    "    a_leg = soup[big_space2 + 2 : leg]\n",
    "    big_space4 = soup.find('  ', big_space2 + 2)\n",
    "    b_leg = soup[leg + 10 : leg + 17]\n",
    "\n",
    "    a_record = a_record.strip()\n",
    "    b_record = b_record.strip()\n",
    "\n",
    "    a_height_ft = float(a_height[:1])\n",
    "    a_height_in = float(a_height[3:].replace(\"'\", \"\").replace('\"', ''))\n",
    "    a_height = (a_height_ft * 12) + a_height_in \n",
    "\n",
    "\n",
    "    b_height_ft = float(b_height[:1])\n",
    "    b_height_in = float(b_height[3:].replace(\"'\", \"\").replace('\"', ''))\n",
    "    b_height = (b_height_ft * 12) + b_height_in\n",
    "\n",
    "    a_reach = float(a_reach.replace(' in', '').strip())\n",
    "    b_reach = float(b_reach.replace(' in', '').strip())\n",
    "\n",
    "    a_leg = float(a_leg.replace(' in', '').strip())\n",
    "    b_leg = float(b_leg.replace(' in', '').strip())\n",
    "\n",
    "    \n",
    "    return a_record, b_record, a_height, b_height, a_reach, b_reach, a_leg, b_leg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "matchup_url = 'https://www.ufc.com/event/ufc-287#10377'\n",
    "a_record, b_record, a_height, b_height, a_reach, b_reach, a_leg, b_leg = grab_matchup_data(matchup_url)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Trav310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f69e36f0e9b2c8d9f319b417484f14b77c91d7bef950ad448542405eb1e0e594"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
