{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is where I put things that may be helpul, but are not needed where I found them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib.ticker as mtick\n",
    "import seaborn as sns\n",
    "from matplotlib.pyplot import figure\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import requests     \n",
    "import shutil      \n",
    "import datetime\n",
    "from scipy.stats import norm\n",
    "\n",
    "from random import randint\n",
    "import  random\n",
    "import os\n",
    "os.chdir('C:/Users/Travis/OneDrive/Data Science/Personal_Projects/Sports/UFC_Prediction_V2')\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup \n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_events_from_ufcstats():\n",
    "\n",
    "    # Returns the titles and URLS of All Completed UFC Events\n",
    "    \n",
    "    ufc_stats_events = \"http://ufcstats.com/statistics/events/completed?page=all\"\n",
    "\n",
    "    # for each event, get the event link\n",
    "    page = requests.get(ufc_stats_events)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    event_linkies = soup.find_all('a', class_='b-link b-link_style_black')\n",
    "\n",
    "    # list of event links and event titles\n",
    "    event_links = [n['href'] for n in event_linkies]\n",
    "    event_titles = [n.text for n in event_linkies]\n",
    "    # replace '\\n' with ''\n",
    "    event_titles = [n.replace('\\n', '').strip() for n in event_titles]\n",
    "\n",
    "    # make df\n",
    "    event_df = pd.DataFrame({'event_title': event_titles, 'event_url': event_links})\n",
    "\n",
    "\n",
    "    return event_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event_title</th>\n",
       "      <th>event_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UFC Fight Night: Thompson vs. Holland</td>\n",
       "      <td>http://ufcstats.com/event-details/b23388ff8ac6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UFC Fight Night: Nzechukwu vs. Cutelaba</td>\n",
       "      <td>http://ufcstats.com/event-details/012fc7cd0779...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UFC 281: Adesanya vs. Pereira</td>\n",
       "      <td>http://ufcstats.com/event-details/b3b6e80b7d5f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>UFC Fight Night: Rodriguez vs. Lemos</td>\n",
       "      <td>http://ufcstats.com/event-details/756f45905fb2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>UFC Fight Night: Kattar vs. Allen</td>\n",
       "      <td>http://ufcstats.com/event-details/be5aab761c40...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>623</th>\n",
       "      <td>UFC 6: Clash of the Titans</td>\n",
       "      <td>http://ufcstats.com/event-details/1c3f5e85b59e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>624</th>\n",
       "      <td>UFC 5: The Return of the Beast</td>\n",
       "      <td>http://ufcstats.com/event-details/dedc3bb440d0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>625</th>\n",
       "      <td>UFC 4: Revenge of the Warriors</td>\n",
       "      <td>http://ufcstats.com/event-details/b60391da771d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>626</th>\n",
       "      <td>UFC 3: The American Dream</td>\n",
       "      <td>http://ufcstats.com/event-details/1a49e0670dfa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>627</th>\n",
       "      <td>UFC 2: No Way Out</td>\n",
       "      <td>http://ufcstats.com/event-details/a6a9ab5a824e...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>628 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 event_title  \\\n",
       "0      UFC Fight Night: Thompson vs. Holland   \n",
       "1    UFC Fight Night: Nzechukwu vs. Cutelaba   \n",
       "2              UFC 281: Adesanya vs. Pereira   \n",
       "3       UFC Fight Night: Rodriguez vs. Lemos   \n",
       "4          UFC Fight Night: Kattar vs. Allen   \n",
       "..                                       ...   \n",
       "623               UFC 6: Clash of the Titans   \n",
       "624           UFC 5: The Return of the Beast   \n",
       "625           UFC 4: Revenge of the Warriors   \n",
       "626                UFC 3: The American Dream   \n",
       "627                        UFC 2: No Way Out   \n",
       "\n",
       "                                             event_url  \n",
       "0    http://ufcstats.com/event-details/b23388ff8ac6...  \n",
       "1    http://ufcstats.com/event-details/012fc7cd0779...  \n",
       "2    http://ufcstats.com/event-details/b3b6e80b7d5f...  \n",
       "3    http://ufcstats.com/event-details/756f45905fb2...  \n",
       "4    http://ufcstats.com/event-details/be5aab761c40...  \n",
       "..                                                 ...  \n",
       "623  http://ufcstats.com/event-details/1c3f5e85b59e...  \n",
       "624  http://ufcstats.com/event-details/dedc3bb440d0...  \n",
       "625  http://ufcstats.com/event-details/b60391da771d...  \n",
       "626  http://ufcstats.com/event-details/1a49e0670dfa...  \n",
       "627  http://ufcstats.com/event-details/a6a9ab5a824e...  \n",
       "\n",
       "[628 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get all events from UFCStats -- not to be used, just for reference\n",
    "\n",
    "all_events = get_all_events_from_ufcstats()\n",
    "all_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_event_name(url):\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    event_name = soup.find('h2', class_='b-content__title').text\n",
    "    # replace \\n with nothing\n",
    "    event_name = event_name.replace('\\n', '')\n",
    "    # replace multiple spaces with nothing\n",
    "    event_name = event_name.replace('  ', '')\n",
    "    return event_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST Of my version of scraping\n",
    "\n",
    "url = 'https://www.google.com/search?query=site:bestfightodds.com Adesanya Pereira' \n",
    "header = {\n",
    "                \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.75 Safari/537.36\",\n",
    "                \"X-Requested-With\": \"XMLHttpRequest\"\n",
    "}\n",
    "r = requests.get(url, headers = header)\n",
    "# get all links\n",
    "soup = BeautifulSoup(r.text, 'html.parser')\n",
    "links = soup.find_all('a')\n",
    "# keep only events\n",
    "goog_response = [n['href'] for n in links if 'events' in n['href']]\n",
    "# clean up the link so it is only the url\n",
    "goog_response = [n.replace('/url?q=', '') for n in goog_response]\n",
    "# cut off the begining up until https\n",
    "goog_response = [n[n.find('https'):] for n in goog_response]\n",
    "# cut off the end after the &\n",
    "goog_response = [n[:n.find('&')] for n in goog_response]\n",
    "# keep only the first one\n",
    "goog_response = goog_response[0]\n",
    "goog_response"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempting to Streamlit Google Automate\n",
    "\n",
    "This works for about 80, until google says \"HEY STOP IT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "driver.get('https://www.google.com/search?query=site:bestfightodds.com Adesanya Pereira')\n",
    "time.sleep(1)\n",
    "# get all links\n",
    "soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "# find div by id search\n",
    "search = soup.find('div', id='search')\n",
    "# find all a tags\n",
    "links = search.find_all('a')\n",
    "# keep only events\n",
    "goog_response = [n['href'] for n in links if 'events' in n['href']]\n",
    "# delete any future events\n",
    "goog_response = [n for n in goog_response if 'future' not in n]\n",
    "# keep the top response\n",
    "goog_response = goog_response[0]\n",
    "goog_response\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def streamlit_google_scrape_BFO(fighter1, fighter2):\n",
    "    try:\n",
    "        url = 'https://www.google.com/search?query=site:bestfightodds.com ' + fighter1 + ' ' + fighter2\n",
    "        driver = webdriver.Chrome()\n",
    "        driver.get(url)\n",
    "        time.sleep(3)\n",
    "        # get all links\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        # find div by id search\n",
    "        search = soup.find('div', id='search')\n",
    "        # find all a tags\n",
    "        links = search.find_all('a')\n",
    "        # keep only events\n",
    "        goog_response = [n['href'] for n in links if 'events' in n['href']]\n",
    "        # delete any future events\n",
    "        goog_response = [n for n in goog_response if 'future' not in n]\n",
    "        # keep the top response\n",
    "        goog_response = goog_response[0]\n",
    "        # close the driver\n",
    "        driver.close()\n",
    "\n",
    "        # TODO: Add a robot checker to see if we are getting checked by google\n",
    "        return goog_response\n",
    "    except:\n",
    "        # close the driver\n",
    "        driver.close()\n",
    "        return np.nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test \n",
    "streamlit_google_scrape_BFO('Thompson', 'Holland')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STREAMLIT Google Search Function -- please work\n",
    "even['streamlit_google_scrape_BFO'] = even.apply(lambda row: streamlit_google_scrape_BFO(row['fighter1'], row['fighter2']), axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all Fightdata gets both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function gets all the data from a fight page on ufcstats.com,\n",
    "# and DOES SOME OTHER SHIT?  \n",
    "# TODO: DO I STILL NEED THIS?\n",
    "\n",
    "\n",
    "def get_all_fightdata(url):\n",
    "    tot = get_fight_totals(url)\n",
    "    ss = get_significant_strikes(url)\n",
    "    # get columns in both df\n",
    "    cols = list(set(tot.columns).intersection(ss.columns))\n",
    "\n",
    "    # merge\n",
    "    df = pd.merge(tot, ss, on=cols, how='left')\n",
    "    df_2 = df.copy()\n",
    "    \n",
    "    df_2['Fighter_A'] = df['Fighter_B']\n",
    "    df_2['Fighter_B'] = df['Fighter_A']\n",
    "    fighter_cols = [n for n in df.columns if 'A' in n] + [n for n in df.columns if 'B' in n]\n",
    "    fighter_cols.remove('Fighter_A')\n",
    "    fighter_cols.remove('Fighter_B')\n",
    "\n",
    "    for col in fighter_cols:\n",
    "        if 'A_' in col:\n",
    "            df_2[col] = df[col.replace('A_', 'B_')]\n",
    "        elif 'B_' in col:\n",
    "            df_2[col] = df[col.replace('B_', 'A_')]\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    difs = [n for n in df_2.columns if 'diff' in n]\n",
    "\n",
    "    for col in difs:\n",
    "        df_2[col] = df_2[col] * -1\n",
    "\n",
    "    df = df.append(df_2, ignore_index=True)\n",
    "    # change all nan to 0\n",
    "    df.fillna(0, inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ufc_bios(url):\n",
    "    try:\n",
    "        page = requests.get(url)\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "        fighter = soup.find('h1', class_='hero-profile__name').text\n",
    "\n",
    "        sigstrike_by_position= soup.find('div', class_='c-stat-3bar__legend').text\n",
    "        # delete all \\n in sigstrike_by_position\n",
    "        ssbp = sigstrike_by_position.replace('\\n', '')  \n",
    "        st = ssbp.find('Standing')\n",
    "        standing = ssbp[st+9:st+17]\n",
    "        # find parentheses\n",
    "        find_par_start = standing.find('(')\n",
    "        find_par_end = standing.find(')')\n",
    "        standing_percent = standing[find_par_start+1 : find_par_end]\n",
    "        standing_nom = standing[:find_par_start].strip()\n",
    "\n",
    "        cl = ssbp.find('Clinch')\n",
    "        clinch = ssbp[cl+7:cl+15]\n",
    "        # find parentheses\n",
    "        find_par_start = clinch.find('(')\n",
    "        find_par_end = clinch.find(')')\n",
    "        clinch_percent = clinch[find_par_start+1 : find_par_end-1]\n",
    "        clinch_nom = clinch[:find_par_start].strip()\n",
    "\n",
    "        gr = ssbp.find('Ground')\n",
    "        ground = ssbp[gr+7:gr+15]\n",
    "        # find parentheses\n",
    "        find_par_start = ground.find('(')\n",
    "        find_par_end = ground.find(')')\n",
    "        ground_percent = ground[find_par_start+1 : find_par_end-1]\n",
    "        ground_nom = ground[:find_par_start].strip()\n",
    "\n",
    "        # bio section scrape\n",
    "        bio = soup.find('div', class_='c-bio__info-details').text\n",
    "        bio = bio.replace('\\n', '')\n",
    "\n",
    "        # Trains at: \n",
    "        # if fighting style is in bio, it will be after trains at\n",
    "\n",
    "        if 'Fighting style' in bio:\n",
    "            ta = bio.find('Trains at')\n",
    "            fs = bio.find('Fighting style')\n",
    "            trains_at = bio[ta+9:fs]\n",
    "            ag = bio.find('Age')\n",
    "            fighting_style = bio[fs+14:ag]\n",
    "        else:\n",
    "            ta = bio.find('Trains at')\n",
    "            ag = bio.find('Age')\n",
    "            trains_at = bio[ta+10:ag]\n",
    "            fighting_style = nan\n",
    "\n",
    "\n",
    "        # Height\n",
    "        ht = bio.find('Height')\n",
    "        wt = bio.find('Weight')\n",
    "        height = bio[ht+6:wt]\n",
    "\n",
    "        # Weight\n",
    "        wt = bio.find('Weight')\n",
    "        oct = bio.find('Octagon Debut')\n",
    "        weight = bio[wt+6:oct]\n",
    "\n",
    "        # Octagon Debut\n",
    "        oct = bio.find('Octagon Debut')\n",
    "        rch = bio.find('Reach')\n",
    "        octagon_debut = bio[oct+13:rch]\n",
    "\n",
    "        # Reach\n",
    "        rch = bio.find('Reach')\n",
    "        lg = bio.find('Leg reach')\n",
    "        reach = bio[rch+5:lg]\n",
    "\n",
    "        # Leg Reach\n",
    "        lg = bio.find('Leg reach')\n",
    "        leg_reach = bio[lg+9:]\n",
    "\n",
    "        data = pd.DataFrame({'Fighter_Name': fighter,'SigStrike_Standing': standing_nom, 'SigStrike_Standing_Percent': standing_percent, \n",
    "                                'SigStrike_Clinch': clinch_nom, 'SigStrike_Clinch_Percent': clinch_percent, 'SigStrike_Ground': ground_nom, \n",
    "                                'SigStrike_Ground_Percent': ground_percent, 'Trains_At': trains_at, 'Fighting_Style': fighting_style, \n",
    "                                'Height': height, 'Weight': weight, 'Octagon_Debut': octagon_debut, 'Reach': reach, 'Leg_Reach': leg_reach}, index=[0])\n",
    "\n",
    "        data.to_csv('data/final/fighters/' + fighter + '.csv')\n",
    "        return data\n",
    "\n",
    "    except:\n",
    "        print('Error with ' + str(url))\n",
    "        \n",
    "        fighter = url[28:].replace('-', ' ').title()\n",
    "        standing_nom = nan\n",
    "        standing_percent = nan\n",
    "        clinch_nom = nan\n",
    "        clinch_percent = nan\n",
    "        ground_nom = nan\n",
    "        ground_percent = nan\n",
    "        trains_at = nan\n",
    "        height = nan\n",
    "        weight = nan\n",
    "        octagon_debut = nan\n",
    "        reach = nan\n",
    "        leg_reach = nan\n",
    "        fighting_style = nan\n",
    "\n",
    "        data = pd.DataFrame({'Fighter_Name': fighter,'SigStrike_Standing': standing_nom, 'SigStrike_Standing_Percent': standing_percent, 'SigStrike_Clinch': clinch_nom, 'SigStrike_Clinch_Percent': clinch_percent, 'SigStrike_Ground': ground_nom, 'SigStrike_Ground_Percent': ground_percent, 'Trains_At': trains_at, 'Fighting_Style': fighting_style, 'Height': height, 'Weight': weight, 'Octagon_Debut': octagon_debut, 'Reach': reach, 'Leg_Reach': leg_reach}, index=[0])\n",
    "        data.to_csv('data/final/fighters/bios_with_errors/' + fighter + '.csv')\n",
    "        return data\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Trav310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 | packaged by conda-forge | (main, Nov  4 2022, 13:42:51) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f69e36f0e9b2c8d9f319b417484f14b77c91d7bef950ad448542405eb1e0e594"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
