{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streamlit Workbook\n",
    "\n",
    "This is for figuring out the kinks in the Streamlit app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cd OneDrive/Data Science/Personal_Projects/Sports/UFC_Prediction/notebooks/final_notebooks\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "import sqlite3\n",
    "import seaborn as sns\n",
    "from matplotlib.pyplot import figure\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import requests     # to get images\n",
    "import shutil       # to save files locally\n",
    "import datetime\n",
    "from scipy.stats import norm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from random import randint\n",
    "import  random\n",
    "import os\n",
    "#os.chdir('C:/Users/tmcro/OneDrive/Data Science/Personal_Projects/Sports/UFC_Prediction')\n",
    "from cmath import nan\n",
    "from bs4 import BeautifulSoup\n",
    "import streamlit as st\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "os.chdir('C:/Users/Travis/OneDrive/Data Science/Personal_Projects/Sports/UFC_Prediction_V2/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event_title</th>\n",
       "      <th>event_url</th>\n",
       "      <th>event_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UFC 283: Teixeira vs. Hill</td>\n",
       "      <td>http://www.ufcstats.com/event-details/5717efc6...</td>\n",
       "      <td>January 21, 2023</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  event_title  \\\n",
       "0  UFC 283: Teixeira vs. Hill   \n",
       "\n",
       "                                           event_url        event_date  \n",
       "0  http://www.ufcstats.com/event-details/5717efc6...  January 21, 2023  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_next_events(url):\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    # get events\n",
    "    event1 = soup.find('div', class_='c-card-event--result__info')\n",
    "    event1_txt = soup.find('div', class_='c-card-event--result__info').text\n",
    "    event1_url = event1.find('a')['href']\n",
    "    event1_url = 'https://www.ufc.com' + event1_url\n",
    "    event1_title = event1_txt.split('\\n')[1]\n",
    "    event1_time = event1_txt.split('/')[1]\n",
    "\n",
    "    data = pd.DataFrame({'event_title': [event1_title], 'event_url': [event1_url], 'event_date': [event1_time]})\n",
    "\n",
    "    event2 = soup.find('div', class_='c-card-event--result__info').find_next('div', class_='c-card-event--result__info')\n",
    "    event2_txt = soup.find('div', class_='c-card-event--result__info').find_next('div', class_='c-card-event--result__info').text\n",
    "    event2_url = event2.find('a')['href']\n",
    "    event2_url = 'https://www.ufc.com' + event2_url\n",
    "    event2_title = event2_txt.split('\\n')[1]\n",
    "    event2_time = event2_txt.split('/')[1]\n",
    "\n",
    "\n",
    "    data = data.append({'event_title': event2_title, 'event_url': event2_url, 'event_date': event2_time}, ignore_index=True)\n",
    "    \n",
    "    event3 = soup.find('div', class_='c-card-event--result__info').find_next('div', class_='c-card-event--result__info').find_next('div', class_='c-card-event--result__info')\n",
    "    event3_txt = soup.find('div', class_='c-card-event--result__info').find_next('div', class_='c-card-event--result__info').find_next('div', class_='c-card-event--result__info').text\n",
    "    event3_url = event3.find('a')['href']\n",
    "    event3_url = 'https://www.ufc.com' + event3_url\n",
    "    event3_title = event3_txt.split('\\n')[1]\n",
    "    event3_time = event3_txt.split('/')[1]\n",
    "\n",
    "    data = data.append({'event_title': event3_title, 'event_url': event3_url, 'event_date': event3_time}, ignore_index=True)\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Function to get the fight card for a given event\n",
    "def get_event_fights(event_url):\n",
    "    page = requests.get(event_url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    # get main card, fight 1\n",
    "\n",
    "    mcn = soup.find_all('li', class_='l-listing__item')\n",
    "    # get num of mc\n",
    "    num_mc = len(mcn)\n",
    "    # for each mc, do the following\n",
    "    data = pd.DataFrame()\n",
    "    n = 0\n",
    "    for i in mcn:\n",
    "        mc = mcn[n]\n",
    "        # fight 1\n",
    "        fighter1= mc.find('div', class_ ='c-listing-fight__corner-name c-listing-fight__corner-name--red').text\n",
    "        fighter1 = fighter1.replace('\\n', ' ')\n",
    "        fighter1 = fighter1.strip()\n",
    "        fighter2 = mc.find('div', class_ ='c-listing-fight__corner-name c-listing-fight__corner-name--blue').text\n",
    "        fighter2 = fighter2.replace('\\n', ' ')\n",
    "        fighter2 = fighter2.strip()\n",
    "        weightclass = mc.find('div', class_='c-listing-fight__class-text').text\n",
    "        fighter1_odds = mc.find('span', class_='c-listing-fight__odds').text\n",
    "        fighter2_odds = mc.find('span', class_='c-listing-fight__odds').find_next('span', class_='c-listing-fight__odds').text\n",
    "        fighter1_odds = fighter1_odds.replace('\\n', '')\n",
    "        fighter2_odds = fighter2_odds.replace('\\n', '')\n",
    "        # fighter odds to float\n",
    "        if (fighter1_odds == '-') :\n",
    "            fighter1_odds = nan\n",
    "        if (fighter2_odds == '-') :\n",
    "            fighter2_odds = nan\n",
    "\n",
    "        data = data.append({'fighter1': fighter1, 'fighter2': fighter2, 'weightclass': weightclass, \n",
    "                            'fighter1_odds': fighter1_odds, 'fighter2_odds': fighter2_odds}, ignore_index=True)\n",
    "        n = n + 1\n",
    "    return data\n",
    "\n",
    "\n",
    "def secret_number(event_url):\n",
    "    # if no driver open, open one\n",
    "    driver = None\n",
    "    if (driver == None):\n",
    "        driver = webdriver.Chrome('C:\\\\Users\\\\Travis\\\\OneDrive\\\\Data Science\\\\Personal_Projects\\\\Sports\\\\UFC_Prediction_V2\\\\chromedriver.exe')\n",
    "    else:\n",
    "        driver = driver\n",
    "    \n",
    "    driver.get(event_url)\n",
    "    time.sleep(3)\n",
    "    # click the first matchup\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    pretty = soup.prettify()\n",
    "    # find first data-fmid to get first matchup\n",
    "    fmid_start = pretty.find('data-fmid')\n",
    "    fmid = pretty[fmid_start+11:fmid_start+16]\n",
    "    driver.get(event_url +'#' + fmid)\n",
    "    time.sleep(6)\n",
    "    # find all links within page\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    # find all iframe src\n",
    "    iframe = soup.find_all('iframe')\n",
    "    # find all links\n",
    "    iframe_text = str(iframe)\n",
    "    matchup = iframe_text.find('matchup')\n",
    "    matchup_url = iframe_text[matchup+8:matchup+12]\n",
    "    print('matchup_url: ' + matchup_url)\n",
    "    secret_number = matchup_url\n",
    "    return secret_number\n",
    "\n",
    "\n",
    "# get next events if event fighter data is not na\n",
    "def get_next_events2(url):\n",
    "    data = get_next_events(url)\n",
    "    for i in range(0, len(data)):\n",
    "        event_url = data['event_url'][i]\n",
    "        event_fights = get_event_fights(event_url)\n",
    "        if (len(event_fights) == 0):\n",
    "            data = data.drop(i)\n",
    "    return data\n",
    "\n",
    "def get_next_event_ufcstats():\n",
    "    url = 'http://www.ufcstats.com/statistics/events/upcoming'\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    # get events\n",
    "    event1 = soup.find('td', class_='b-statistics__table-col')\n",
    "    event1_txt = soup.find('td', class_='b-statistics__table-col').text\n",
    "    event_txt = event1_txt.replace('   ', '').replace('\\n', '').strip()\n",
    "    event_title = event_txt.split('  ')[0]\n",
    "    event_date = event_txt.split('  ')[1]\n",
    "    event1_url = event1.find('a')['href']\n",
    "    data = pd.DataFrame({'event_title': [event_title], 'event_url': [event1_url], 'event_date': [event_date]})\n",
    "    return data\n",
    "\n",
    "def get_fighter_urls(event_details_url):\n",
    "    page = requests.get(event_details_url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    # get events\n",
    "    events = soup.find_all('tr', class_='b-fight-details__table-row b-fight-details__table-row__hover js-fight-details-click')\n",
    "    n = 0\n",
    "    next_event_data = pd.DataFrame()\n",
    "\n",
    "    for event in events:\n",
    "        fighters = events[n].find_all('p', class_='b-fight-details__table-text')\n",
    "        fighter1 = fighters[0].text\n",
    "        fighter1 = fighter1.replace('  ', '').replace('\\n', '').strip()\n",
    "        fighter2 = fighters[1].text\n",
    "        fighter2 = fighter2.replace('  ', '').replace('\\n', '').strip()\n",
    "        fighter1_url = fighters[0].find('a')['href']\n",
    "        fighter2_url = fighters[1].find('a')['href']\n",
    "        next_event_data = next_event_data.append({'fighter1' :fighter1, 'fighter2:' : fighter2, 'fighter1_url': fighter1_url, 'fighter2_url':fighter2_url, 'fight#' : n+1}, ignore_index = True)\n",
    "        n += 1\n",
    "\n",
    "    return next_event_data\n",
    "\n",
    "\n",
    "\n",
    "# check if it is a saturday\n",
    "def is_saturday():\n",
    "    today = str(datetime.today().weekday())\n",
    "    if today == '5':\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# if it is a saturday, get the current event, otherwise, get the next event\n",
    "\n",
    "\n",
    "next_eventz = get_next_event_ufcstats()\n",
    "\n",
    "# print the day of the week\n",
    "next_eventz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matchup_url: 1127\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://www.ufc.com/event/ufc-fight-night-january-14-2023#10315'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "##########           DATA        ################\n",
    "#data = pd.read_csv(home + '/final/aggregates/Double_Fights_DF_V14.csv')\n",
    "\n",
    "##########           GET EVENTS       ################\n",
    "\n",
    "# make sure events have fight info. If not, disregard that event\n",
    "next = get_next_events2('https://www.ufc.com/events')\n",
    "\n",
    "########           Select Next Event    ################\n",
    "\n",
    "event = next['event_title'][0]\n",
    "selected_event = event\n",
    "event_url =  next['event_url'][next['event_title'] == selected_event].values[0]\n",
    "selected_event_secret_number = secret_number(event_url)\n",
    "\n",
    "next_event = get_event_fights(event_url)\n",
    "fight = st.sidebar.selectbox('Select Fight', next_event['fighter1'] + ' vs. ' + next_event['fighter2'])\n",
    "\n",
    "## Get Names ##\n",
    "\n",
    "selected_fighter_1 = fight.split(' vs. ')[0]\n",
    "selected_fighter_2 = fight.split(' vs. ')[1].strip()\n",
    "\n",
    "\n",
    "########          Scrape UFC.com Data    ################\n",
    "\n",
    "# get the matchup fight numbers\n",
    "\n",
    "page = requests.get(event_url)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "h = soup.find_all('div', class_='c-listing-fight')\n",
    "data_fmid = []\n",
    "for i in h:\n",
    "    data_fmid.append(i['data-fmid'])\n",
    "\n",
    "next_event['fight_number'] = data_fmid[:len(next_event)]\n",
    "next_event['matchup_url'] = event_url +'#' + next_event['fight_number'].astype(str)\n",
    "selected_matchup_url = next_event['matchup_url'][next_event['fighter1'] == selected_fighter_1].values[0]\n",
    "\n",
    "#st.write(' FOR EDIT -- selected_matchup_url: ' + selected_matchup_url)\n",
    "selected_matchup_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to scrape UFC fight data\n",
    "def grab_matchup_data(matchup_url):\n",
    "    response = requests.get(matchup_url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser').text\n",
    "    soup = soup.replace('   ', '').replace('\\n', '')\n",
    "    \n",
    "    # get img src\n",
    "    \n",
    "\n",
    "    od = soup.find('Odds')\n",
    "    rec = soup.find('Record')\n",
    "    a_record = soup[od + 5 : rec - 2]\n",
    "    last_fight = soup.find(\"Last Fight\")\n",
    "    b_record = soup[rec + 7 : last_fight - 5]\n",
    "\n",
    "    hite = soup.find('Height')\n",
    "    f = soup.find(\"' \")\n",
    "    a_height = soup[f -1 : hite - 2]\n",
    "    # find second occurance of f\n",
    "    f2 = soup.find(\"' \", f + 1)\n",
    "    b_height = soup[hite + 7 : f2+5]\n",
    "\n",
    "    # Find reach\n",
    "    reach = soup.find('Reach')\n",
    "    # find second occurance of \"LB\"\n",
    "    lb = soup.find('LB')\n",
    "    lb2 = soup.find('LB', lb + 1)\n",
    "    a_reach = soup[lb2 +5 : reach ]\n",
    "    inn = soup.find(\"in \")\n",
    "    # get the word after reach\n",
    "    big_space = soup.find('  ', reach + 1)\n",
    "    b_reach = soup[reach + 6 : big_space]\n",
    "\n",
    "    # Find Leg Reach\n",
    "    leg = soup.find('Leg Reach')\n",
    "    big_space2 = soup.find('  ', big_space + 1)\n",
    "    a_leg = soup[big_space2 + 2 : leg]\n",
    "    big_space4 = soup.find('  ', big_space2 + 2)\n",
    "    b_leg = soup[leg + 10 : leg + 17]\n",
    "\n",
    "    a_record = a_record.strip()\n",
    "    b_record = b_record.strip()\n",
    "\n",
    "    a_height_ft = float(a_height[:1])\n",
    "    a_height_in = float(a_height[3:].replace(\"'\", \"\").replace('\"', ''))\n",
    "    a_height = (a_height_ft * 12) + a_height_in \n",
    "\n",
    "\n",
    "    b_height_ft = float(b_height[:1])\n",
    "    b_height_in = float(b_height[3:].replace(\"'\", \"\").replace('\"', ''))\n",
    "    b_height = (b_height_ft * 12) + b_height_in\n",
    "\n",
    "    a_reach = float(a_reach.replace(' in', '').strip())\n",
    "    b_reach = float(b_reach.replace(' in', '').strip())\n",
    "\n",
    "    a_leg = float(a_leg.replace(' in', '').strip())\n",
    "    b_leg = float(b_leg.replace(' in', '').strip())\n",
    "\n",
    "    \n",
    "    return a_record, b_record, a_height, b_height, a_reach, b_reach, a_leg, b_leg, img_url\n",
    "\n",
    "url = 'https://www.ufc.com/matchup/' + selected_event_secret_number + '/' + next_event[next_event['fighter1'] == selected_fighter_1]['fight_number'].values[0]\n",
    "\n",
    "a_record, b_record, a_height, b_height, a_reach, b_reach, a_leg, b_leg, img_url = grab_matchup_data(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'25-5-0'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'STRICKLAND'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fighter_last_name = selected_fighter_1.split(' ')[1]\n",
    "fighter_last_name = fighter_last_name.upper()\n",
    "fighter_last_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://dmxg5wxfqgb4u.cloudfront.net/styles/athlete_detail_stance_thumbnail_full_body/s3/2023-01/STRICKLAND_SEAN_L_12-17.png?itok=U-pjxH3U'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "driver.get(selected_matchup_url)\n",
    "    \n",
    "# wait 10 seconds\n",
    "time.sleep(3)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "h = soup.find_all('img')\n",
    "imgs = []\n",
    "for i in h:\n",
    "    imgs.append(i['src'])\n",
    "\n",
    "# keep imgs with full_body\n",
    "imgs = [i for i in imgs if 'full_body' in i]\n",
    "\n",
    "fighter_img = [i for i in imgs if fighter_last_name in i][0]\n",
    "fighter_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://dmxg5wxfqgb4u.cloudfront.net/styles/athlete_matchup_stats_full_body/s3/2023-01/STRICKLAND_SEAN_L_12-17.png'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# in fighter_img, replace athlete_detail_stance_thumbnail_full_body with athlete_matchup_stats_full_body\n",
    "fighter_img = fighter_img.replace('athlete_detail_stance_thumbnail_full_body', 'athlete_matchup_stats_full_body')\n",
    "# end fighter_img at .png\n",
    "fighter_img = fighter_img[:fighter_img.find('.png') + 4]\n",
    "fighter_img"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Trav310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 | packaged by conda-forge | (main, Nov  4 2022, 13:42:51) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f69e36f0e9b2c8d9f319b417484f14b77c91d7bef950ad448542405eb1e0e594"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
